api: paddle.nn.SyncBatchNorm
constraints:
  bias_attr:
    default: None
    descp: 'The parameter attribute for the bias of this layer. If it is set to None
      or one attribute of ParamAttr, this layer will create ParamAttr as bias_attr.
      If the Initializer of the bias_attr is not set, the bias is initialized zero.
      If it is set to False, this layer will not have trainable bias parameter. Default:
      None'
    dtype:
    - bool
    - ParamAttr
    enum: null
    range: null
    shape: null
    structure:
    - ParamAttr
    - bool
  epsilon:
    default: 1e-05
    descp: 'The small value added to the variance to prevent division by zero. Default:
      1e-5'
    dtype:
    - float
    enum: null
    range: null
    shape: null
    structure:
    - float
  momentum:
    default: '0.9'
    descp: 'The value used for the moving_mean and moving_var computation. Default:
      0'
    dtype:
    - float
    enum: null
    range: null
    shape: null
    structure:
    - float
  num_features:
    default: null
    descp: Indicate the number of channels of the input Tensor
    dtype:
    - int
    enum: null
    range: null
    shape: null
    structure:
    - int
  weight_attr:
    default: None
    descp: 'The parameter attribute for Parameter scale of this layer. If it is set
      to None or one attribute of ParamAttr, this layerr will create ParamAttr as
      param_attr. If the Initializer of the param_attr is not set, the parameter is
      initialized with ones. If it is set to False, this layer will not have trainable
      scale parameter. Default: None'
    dtype:
    - bool
    - ParamAttr
    enum: null
    range: null
    shape: null
    structure:
    - ParamAttr
    - bool
descp: 'This interface is used to construct a callable object of the SyncBatchNorm
  class. It implements the function of the Cross-GPU Synchronized Batch Normalization
  Layer, and can be used as a normalizer function for other operations, such as conv2d
  and fully connected operations. The data is normalized by the mean and variance
  of the channel based on whole mini-batch , which including data in all gpus. Refer
  to Batch Normalization: Accelerating Deep Network Training by Reducing Internal
  Covariate Shift for more details.'
inputs:
  optional:
  - momentum
  - epsilon
  - weight_attr
  - bias_attr
  - data_format
  - name
  required:
  - num_features
