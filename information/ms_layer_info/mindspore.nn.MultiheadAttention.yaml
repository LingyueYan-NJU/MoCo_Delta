api: mindspore.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, has_bias=True,
  add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=False)
descp: This is an implementation of multihead attention in the paper Attention is
  all you need.
constraints:
  embed_dim:
    descp: embed_dim (int) – Total dimension of MultiheadAttention.
    default: null
    dtype:
    - int
    structure:
    - single
    range: null
  num_heads:
    descp: num_heads (int) – Number of attention heads. Note that embed_dim will be
      split across num_heads (i.e. each head will have dimension embed_dim // num_heads).
    default: null
    dtype:
    - int
    structure:
    - single
    range: null
  dropout:
    descp: 'dropout (float) – Dropout probability of attn_output_weights. Default:
      0.0.'
    default: 0.0
    dtype:
    - float
    structure:
    - single
  has_bias:
    descp: 'has_bias (bool) – Whether adds bias to input / output projection layers.
      Default: True.'
    default: True
    dtype:
    - bool
  add_bias_kv:
    descp: 'add_bias_kv (bool) – Whether adds bias to the key and value sequences
      at axis=0. Default: False.'
    default: False
    dtype:
    - bool
  add_zero_attn:
    descp: 'add_zero_attn (bool) – Whether adds a new batch of zeros to the key and
      value sequences at axis=1. Default: False.'
    default: False
    dtype:
    - bool
  kdim:
    descp: 'kdim (int) – Total number of features for keys. Default: None (kdim=embed_dim).'
    default: None (kdim=embed_dim)
    dtype:
    - int
    structure:
    - single
    range: null
  vdim:
    descp: 'vdim (int) – Total number of features for values. Default: None (vdim=embed_dim).'
    default: None (vdim=embed_dim)
    dtype:
    - int
    structure:
    - single
    range: null
  batch_first:
    descp: 'batch_first (bool) – If True, then the input and output shape are ((batch,
      seq, feature)) , else ((seq, batch, feature)) . Default: False.'
    default: False
    dtype:
    - bool
inputs:
  optional:
  - dropout
  - has_bias
  - add_bias_kv
  - add_zero_attn
  - kdim
  - vdim
  - batch_first
  required:
  - embed_dim
  - num_heads
