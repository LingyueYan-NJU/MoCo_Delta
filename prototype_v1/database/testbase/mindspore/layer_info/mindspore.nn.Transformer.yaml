api: 'mindspore.nn.Transformer(d_model: int = 512, nhead: int = 8, num_encoder_layers:
  int = 6, num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float
  = 0.1, activation: Union[str, Cell, callable] = ''relu'', custom_encoder: Optional[Cell]
  = None, custom_decoder: Optional[Cell] = None, layer_norm_eps: float = 1e-05, batch_first:
  bool = False, norm_first: bool = False)'
descp: Transformer module including encoder and decoder.
constraints:
  d_model:
    descp: 'd_model (int) – The number of expected features in the inputs tensor.
      Default: 512.'
    default: 512
    dtype:
    - int
    structure:
    - single
    range: null
  nhead:
    descp: 'nhead (int) – The number of heads in the MultiheadAttention modules. Default:
      8.'
    default: 8
    dtype:
    - int
    structure:
    - single
    range: null
  num_encoder_layers:
    descp: 'num_encoder_layers (int) – The number of encoder-layers in the encoder.
      Default: 6.'
    default: 6
    dtype:
    - int
    structure:
    - single
    range: null
  num_decoder_layers:
    descp: 'num_decoder_layers (int) – The number of decoder-layers in the decoder.
      Default: 6.'
    default: 6
    dtype:
    - int
    structure:
    - single
    range: null
  dim_feedforward:
    descp: 'dim_feedforward (int) – The dimension of the feedforward layer. Default:
      2048.'
    default: 2048
    dtype:
    - int
    structure:
    - single
    range: null
  dropout:
    descp: 'dropout (float) – The dropout value. Default: 0.1.'
    default: 0.1
    dtype:
    - float
  activation:
    descp: 'activation (Union[str, callable, Cell]) – The activation function of the
      intermediate layer, can be a string (“relu” or “gelu”), Cell instance (nn.ReLU()
      or nn.GELU()) or a callable (ops.relu or ops.gelu). Default: "relu"'
    default: relu
    dtype:
    - str
    enum:
    - relu
    - gelu
  custom_encoder:
    descp: 'custom_encoder (Cell) – Custom encoder. Default: None.'
    default: None
    dtype:
    - cell
  custom_decoder:
    descp: 'custom_decoder (Cell) – Custom decoder. Default: None.'
    default: None
    dtype:
    - cell
  layer_norm_eps:
    descp: 'layer_norm_eps (float) – the epsilion value in layer normalization module.
      Default: 1e-5.'
    default: 1e-5
    dtype:
    - float
  batch_first:
    descp: 'batch_first (bool) – If batch_first = True, then the shape of input and
      output tensors is ((batch, seq, feature)) , otherwise the shape is ((seq, batch,
      feature)) . Default: False.'
    default: False
    dtype:
    - bool
  norm_first:
    descp: 'norm_first (bool) – If norm_first = True, layer norm is done prior to
      attention and feedforward operations, respectively. Default: False.'
    default: False
    dtype:
    - bool
inputs:
  optional:
  - d_model
  - nhead
  - num_encoder_layers
  - num_decoder_layers
  - dim_feedforward
  - dropout
  - activation
  - custom_encoder
  - custom_decoder
  - layer_norm_eps
  - batch_first
  - norm_first
  required: []
