api: torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0,
  scale_grad_by_freq=False, mode='mean', sparse=False, _weight=None, include_last_offset=False,
  padding_idx=None, device=None, dtype=None)
constraints:
  embedding_dim:
    descp: the size of each embedding vector
    dtype: int
    structure:
    - integer
    shape: 1
    range:
    - 1
    - 1026
  include_last_offset:
    descp: if True,
    default: false
    dtype: torch.bool
  max_norm:
    descp: If given, each embedding vector with norm larger than
    dtype: torch.float
  mode:
    descp: '"sum", "mean" or "max". Specifies the way to reduce the bag. "sum" computes
      the weighted sum, taking "mean" computes the average of the values in the bag,
      "max" computes the max value over each bag.'
    default: mean
    dtype: torch.string
    range:
    - sum
    - max
    - mean
  norm_type:
    descp: The p of the p-norm to compute for the 2.
    default: 2.0
    dtype: torch.float
  num_embeddings:
    descp: size of the dictionary of embeddings
    dtype: int
    structure:
    - integer
    shape: 1
    range:
    - 1
    - 1024
  scale_grad_by_freq:
    descp: if given, this will scale gradients by the inverse of frequency of the
      words in the mini-batch.
    default: false
    dtype: torch.bool
  sparse:
    descp: if True, gradient w.r.t. mode="max".
    default: false
    dtype: torch.bool
descp: "Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating\
  \ the intermediate embeddings."
inputs:
  optional:
  - max_norm
  - norm_type
  - scale_grad_by_freq
  - mode
  - sparse
  - include_last_offset
  required:
  - num_embeddings
  - embedding_dim
