api: torch.nn.MultiheadAttention(class , embed_dim, num_heads, dropout=0.0, bias=True,
  add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=False,
  device=None, dtype=None)
constraints:
  embed_dim:
    descp: Total dimension of the model.
  num_heads:
    descp: Number of parallel attention heads. Note that embed_dim will be split across
      num_heads (i.e. each head will have dimension embed_dim // num_heads).
  dropout:
    descp: Dropout probability on attn_output_weights.
    default: 0.0 (no dropout).
  bias:
    descp: If specified, adds bias to input / output projection layers.
    default: True.
  add_bias_kv:
    descp: If specified, adds bias to the key and value sequences at dim=0.
    default: False.
  add_zero_attn:
    descp: If specified, adds a new batch of zeros to the key and value sequences
      at dim=1.
    default: False.
  kdim:
    descp: Total number of features for keys.
    default: None (uses kdim=embed_dim).
  vdim:
    descp: Total number of features for values.
    default: None (uses vdim=embed_dim).
  batch_first:
    descp: If True, then the input and output tensors are provided as (batch, seq,
      feature).
    default: False (seq, batch, feature).
descp: 'Allows the model to jointly attend to information from different representation
  subspaces as described in the paper: Attention Is All You Need.'
inputs:
  optional: []
  required:
  - embed_dim
  - num_heads
  - dropout
  - bias
  - add_bias_kv
  - add_zero_attn
  - kdim
  - vdim
  - batch_first
