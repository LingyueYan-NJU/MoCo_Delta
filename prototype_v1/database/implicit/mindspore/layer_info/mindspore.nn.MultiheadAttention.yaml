api: mindspore.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, has_bias=True,
  add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=False)
constraints:
  add_bias_kv:
    default: false
    descp: "add_bias_kv (bool) \u2013 Whether adds bias to the key and value sequences\
      \ at axis=0. Default: False."
    dtype:
    - bool
  add_zero_attn:
    default: false
    descp: "add_zero_attn (bool) \u2013 Whether adds a new batch of zeros to the key\
      \ and value sequences at axis=1. Default: False."
    dtype:
    - bool
  batch_first:
    default: false
    descp: "batch_first (bool) \u2013 If True, then the input and output shape are\
      \ ((batch, seq, feature)) , else ((seq, batch, feature)) . Default: False."
    dtype:
    - bool
  dropout:
    default: 0.0
    descp: "dropout (float) \u2013 Dropout probability of attn_output_weights. Default:\
      \ 0.0."
    dtype:
    - float
    structure:
    - single
  embed_dim:
    default: null
    descp: "embed_dim (int) \u2013 Total dimension of MultiheadAttention."
    dtype:
    - int
    range: null
    structure:
    - single
  has_bias:
    default: true
    descp: "has_bias (bool) \u2013 Whether adds bias to input / output projection\
      \ layers. Default: True."
    dtype:
    - bool
  kdim:
    default: None (kdim=embed_dim)
    descp: "kdim (int) \u2013 Total number of features for keys. Default: None (kdim=embed_dim)."
    dtype:
    - int
    range: null
    structure:
    - single
  num_heads:
    default: null
    descp: "num_heads (int) \u2013 Number of attention heads. Note that embed_dim\
      \ will be split across num_heads (i.e. each head will have dimension embed_dim\
      \ // num_heads)."
    dtype:
    - int
    range: null
    structure:
    - single
  vdim:
    default: None (vdim=embed_dim)
    descp: "vdim (int) \u2013 Total number of features for values. Default: None (vdim=embed_dim)."
    dtype:
    - int
    range: null
    structure:
    - single
descp: This is an implementation of multihead attention in the paper Attention is
  all you need.
inputs:
  optional:
  - dropout
  - has_bias
  - add_bias_kv
  - add_zero_attn
  - kdim
  - vdim
  - batch_first
  required:
  - embed_dim
  - num_heads
