api: mindspore.nn.optim_ex.SGD(params, lr, momentum=0, dampening=0, weight_decay=0,
  nesterov=False, *, maximize=False)
constraints:
  dampening:
    default: 0
    descp: "dampening (Union[int, float], optional) \u2013 dampening for momentum.\
      \ Default: 0."
    dtype:
    - int
    - float
    range: null
    structure:
    - single
  lr:
    default: null
    descp: "lr (Union[int, float, Tensor]) \u2013 learning rate."
    dtype:
    - int
    - float
    range: null
    structure:
    - single
  momentum:
    default: 0
    descp: "momentum (Union[int, float], optional) \u2013 momentum factor. Default:\
      \ 0."
    dtype:
    - int
    - float
    range: null
    structure:
    - single
  nesterov:
    default: false
    descp: "nesterov (bool, optional) \u2013 enables Nesterov momentum. Default: False."
    dtype:
    - bool
  params:
    default: null
    descp: "params (Union[list(Parameter), list(dict)]) \u2013 list of parameters\
      \ to optimize or dicts defining parameter groups."
    dtype:
    - union[list(parameter), list(dict)]
    structure:
    - list
  weight_decay:
    default: 0
    descp: "weight_decay (float, optional) \u2013 weight decay (L2 penalty). Default:\
      \ 0."
    dtype:
    - float
descp: Stochastic Gradient Descent optimizer.
inputs:
  optional:
  - momentum
  - weight_decay
  - dampening
  - nesterov
  required:
  - params
  - lr
