api: mindspore.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.9, affine=True,
  gamma_init='ones', beta_init='zeros', moving_mean_init='zeros', moving_var_init='ones',
  use_batch_statistics=None)
constraints:
  affine:
    default: true
    descp: "affine (bool) \u2013 A bool value. When set to True , gamma and beta can\
      \ be learned. Default: True ."
    dtype:
    - bool
  beta_init:
    default: zeros
    descp: "beta_init (Union[Tensor, str, Initializer, numbers.Number]) \u2013 Initializer\
      \ for the beta weight. The values of str refer to the function initializer including\
      \ 'zeros' , 'ones' , etc. Default: 'zeros' ."
    dtype:
    - str
    enum:
    - zeros
    - ones
  eps:
    default: 1e-5
    descp: "eps (float) \u2013 A value added to the denominator for numerical stability.\
      \ Default: 1e-5 ."
    dtype:
    - float
  gamma_init:
    default: ones
    descp: "gamma_init (Union[Tensor, str, Initializer, numbers.Number]) \u2013 Initializer\
      \ for the gamma weight. The values of str refer to the function initializer\
      \ including 'zeros' , 'ones' , etc. Default: 'ones' ."
    dtype:
    - str
    enum:
    - zeros
    - ones
  momentum:
    default: 0.9
    descp: "momentum (float) \u2013 A floating hyperparameter of the momentum for\
      \ the running_mean and running_var computation. Default: 0.9 ."
    dtype:
    - float
  moving_mean_init:
    default: zeros
    descp: "moving_mean_init (Union[Tensor, str, Initializer, numbers.Number]) \u2013\
      \ Initializer for the moving mean. The values of str refer to the function initializer\
      \ including 'zeros' , 'ones' , etc. Default: 'zeros' ."
    dtype:
    - str
    enum:
    - zeros
    - ones
  moving_var_init:
    default: ones
    descp: "moving_var_init (Union[Tensor, str, Initializer, numbers.Number]) \u2013\
      \ Initializer for the moving variance. The values of str refer to the function\
      \ initializer including 'zeros' , 'ones' , etc. Default: 'ones' ."
    dtype:
    - str
    enum:
    - zeros
    - ones
  num_features:
    default: null
    descp: "num_features (int) \u2013 C from an expected input of size ((N, C, D,\
      \ H, W)) ."
    dtype:
    - int
    range: null
    structure:
    - single
  use_batch_statistics:
    default: None
    descp: "use_batch_statistics (bool) \u2013 If true, use the mean value and variance\
      \ value of current batch data. If false, use the mean value and variance value\
      \ of specified value. If None , the training process will use the mean and variance\
      \ of current batch data and track the running mean and variance, the evaluation\
      \ process will use the running mean and variance. Default: None ."
    dtype:
    - bool
descp: Batch Normalization is widely used in convolutional networks. This layer applies
  Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel
  dimension) to avoid internal covariate shift.
inputs:
  optional:
  - eps
  - momentum
  - affine
  - gamma_init
  - beta_init
  - moving_mean_init
  - moving_var_init
  - use_batch_statistics
  required:
  - num_features
