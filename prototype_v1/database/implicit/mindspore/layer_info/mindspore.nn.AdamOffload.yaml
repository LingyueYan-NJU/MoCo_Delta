api: mindspore.nn.AdamOffload(params, learning_rate=0.001, beta1=0.9, beta2=0.999,
  eps=1e-08, use_locking=False, use_nesterov=False, weight_decay=0.0, loss_scale=1.0)
constraints:
  beta1:
    default: 0.9
    descp: "beta1 (float) \u2013 The exponential decay rate for the 1st moment estimations.\
      \ Should be in range (0.0, 1.0). Default: 0.9 ."
    dtype:
    - float
  beta2:
    default: 0.999
    descp: "beta2 (float) \u2013 The exponential decay rate for the 2nd moment estimations.\
      \ Should be in range (0.0, 1.0). Default: 0.999 ."
    dtype:
    - float
  eps:
    default: 1e-8
    descp: "eps (float) \u2013 Term added to the denominator to improve numerical\
      \ stability. Should be greater than 0. Default: 1e-8 ."
    dtype:
    - float
  learning_rate:
    default: 1e-3
    descp: "learning_rate (Union[float, int, Tensor, Iterable, LearningRateSchedule])\
      \ \u2013 Default: 1e-3 .  float: The fixed learning rate value. Must be equal\
      \ to or greater than 0. int: The fixed learning rate value. Must be equal to\
      \ or greater than 0. It will be converted to float. Tensor: Its value should\
      \ be a scalar or a 1-D vector. For scalar, fixed learning rate will be applied.\
      \ For vector, learning rate is dynamic, then the i-th step will take the i-th\
      \ value as the learning rate. Iterable: Learning rate is dynamic. The i-th step\
      \ will take the i-th value as the learning rate. LearningRateSchedule: Learning\
      \ rate is dynamic. During training, the optimizer calls the instance of LearningRateSchedule\
      \ with step as the input to get the learning rate of current step.  "
    dtype:
    - int
    - float
    range: null
    shape: null
    structure:
    - single
  loss_scale:
    default: 1.0
    descp: "loss_scale (float) \u2013 A floating point value for the loss scale. Should\
      \ be greater than 0. In general, use the default value. Only when FixedLossScaleManager\
      \ is used for training and the drop_overflow_update in FixedLossScaleManager\
      \ is set to False , then this value needs to be the same as the loss_scale in\
      \ FixedLossScaleManager. Refer to class mindspore.amp.FixedLossScaleManager\
      \ for more details. Default: 1.0 ."
    dtype:
    - float
  params:
    default: null
    descp: "params (Union[list[Parameter], list[dict]]) \u2013 Must be list of Parameter\
      \ or list of dict. When the params is a list of dict, the string \u201Cparams\u201D\
      , \u201Clr\u201D, \u201Cweight_decay\u201D, and \u201Corder_params\u201D are\
      \ the keys can be parsed.  params: Required. Parameters in current group. The\
      \ value must be a list of Parameter. lr: Optional. If \u201Clr\u201D in the\
      \ keys, the value of corresponding learning rate will be used. If not, the learning_rate\
      \ in optimizer will be used. Fixed and dynamic learning rate are supported.\
      \ weight_decay: Optional. If \u201Cweight_decay\u201D in the keys, the value\
      \ of corresponding weight decay will be used. If not, the weight_decay in the\
      \ optimizer will be used. It should be noted that weight decay can be a constant\
      \ value or a Cell. It is a Cell only when dynamic weight decay is applied. Dynamic\
      \ weight decay is similar to dynamic learning rate, users need to customize\
      \ a weight decay schedule only with global step as input, and during training,\
      \ the optimizer calls the instance of WeightDecaySchedule to get the weight\
      \ decay value of current step. order_params: Optional. When parameters is grouped,\
      \ this usually is used to maintain the order of parameters that appeared in\
      \ the network to improve performance. The value should be parameters whose order\
      \ will be followed in optimizer. If order_params in the keys, other keys will\
      \ be ignored and the element of \u2018order_params\u2019 must be in one group\
      \ of params.  "
    dtype:
    - union[list[parameter], list[dict]]
    structure:
    - list
  use_locking:
    default: false
    descp: "use_locking (bool) \u2013 Whether to enable a lock to protect the updating\
      \ process of variable tensors. If true , updates of the w, m, and v tensors\
      \ will be protected by a lock. If false , the result is unpredictable. Default:\
      \ False ."
    dtype:
    - bool
  use_nesterov:
    default: false
    descp: "use_nesterov (bool) \u2013 Whether to use Nesterov Accelerated Gradient\
      \ (NAG) algorithm to update the gradients. If true , update the gradients using\
      \ NAG. If false , update the gradients without using NAG. Default: False ."
    dtype:
    - bool
  weight_decay:
    default: 0.0
    descp: "weight_decay (Union[float, int, Cell]) \u2013 Weight decay (L2 penalty).\
      \ Default: 0.0 .  float: The fixed weight decay value. Must be equal to or greater\
      \ than 0. int: The fixed weight decay value. Must be equal to or greater than\
      \ 0. It will be converted to float. Cell: Weight decay is dynamic. During training,\
      \ the optimizer calls the instance of the Cell with step as the input to get\
      \ the weight decay value of current step.  "
    dtype:
    - int
    - float
    - cell
    range: null
    shape: null
    structure:
    - single
descp: This optimizer will offload Adam optimizer to host CPU and keep parameters
  being updated on the device, to minimize the memory cost.
inputs:
  optional:
  - learning_rate
  - beta1
  - beta2
  - eps
  - use_locking
  - use_nesterov
  - weight_decay
  - loss_scale
  required:
  - params
