api: mindspore.nn.StepLR(optimizer, step_size, gamma=0.5, last_epoch=- 1, verbose=False)
constraints:
  gamma:
    default: 0.1
    descp: "gamma (float, optional) \u2013 Multiplicative factor of learning rate\
      \ decay. Default: 0.1."
    dtype:
    - float
  last_epoch:
    default: -1
    descp: "last_epoch (int, optional) \u2013 The index of last epoch. Default: -1."
    dtype:
    - int
    range: null
    structure:
    - single
  optimizer:
    default: null
    descp: "optimizer (mindspore.nn.optim_ex.Optimizer) \u2013 Wrapped optimizer."
    dtype:
    - mindspore.nn.optim_ex.optimizer
  step_size:
    default: null
    descp: "step_size (int) \u2013 Period of learning rate decay."
    dtype:
    - int
    range: null
    structure:
    - single
  verbose:
    default: false
    descp: "verbose (bool, optional) \u2013 If True, prints a message to stdout for\
      \ each update. Default: False."
    dtype:
    - bool
descp: Decays the learning rate of each parameter group by gamma every step_size epochs.
inputs:
  optional:
  - gamma
  - last_epoch
  - verbose
  required:
  - optimizer
  - step_size
