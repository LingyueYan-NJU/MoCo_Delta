api: mindspore.nn.Adagrad(params, accum=0.1, learning_rate=0.001, update_slots=True,
  loss_scale=1.0, weight_decay=0.0)
constraints:
  accum:
    default: "0\u30021"
    descp: "accum (float) \u2013 The starting value for (h), must be zero or positive\
      \ values. Default: 0.1 ."
    dtype:
    - float
  learning_rate:
    default: 0.001
    descp: "learning_rate (Union[float, int, Tensor, Iterable, LearningRateSchedule])\
      \ \u2013 Default: 0.001 .  float: The fixed learning rate value. Must be equal\
      \ to or greater than 0. int: The fixed learning rate value. Must be equal to\
      \ or greater than 0. It will be converted to float. Tensor: Its value should\
      \ be a scalar or a 1-D vector. For scalar, fixed learning rate will be applied.\
      \ For vector, learning rate is dynamic, then the i-th step will take the i-th\
      \ value as the learning rate. Iterable: Learning rate is dynamic. The i-th step\
      \ will take the i-th value as the learning rate. LearningRateSchedule: Learning\
      \ rate is dynamic. During training, the optimizer calls the instance of LearningRateSchedule\
      \ with step as the input to get the learning rate of current step.  "
    dtype:
    - int
    - float
    range: null
    shape: null
    structure:
    - single
  loss_scale:
    default: 1.0
    descp: "loss_scale (float) \u2013 Value for the loss scale. It must be greater\
      \ than 0.0. In general, use the default value. Only when FixedLossScaleManager\
      \ is used for training and the drop_overflow_update in FixedLossScaleManager\
      \ is set to False, then this value needs to be the same as the loss_scale in\
      \ FixedLossScaleManager. Refer to class mindspore.amp.FixedLossScaleManager\
      \ for more details. Default: 1.0 ."
    dtype:
    - float
  params:
    default: null
    descp: "params (Union[list[Parameter], list[dict]]) \u2013 Must be list of Parameter\
      \ or list of dict. When the params is a list of dict, the string \u201Cparams\u201D\
      , \u201Clr\u201D, \u201Cweight_decay\u201D, \u201Cgrad_centralization\u201D\
      \ and \u201Corder_params\u201D are the keys can be parsed.  params: Required.\
      \ Parameters in current group. The value must be a list of Parameter. lr: Optional.\
      \ If \u201Clr\u201D in the keys, the value of corresponding learning rate will\
      \ be used. If not, the learning_rate in optimizer will be used. Fixed and dynamic\
      \ learning rate are supported. weight_decay: Optional. If \u201Cweight_decay\u201D\
      \ in the keys, the value of corresponding weight decay will be used. If not,\
      \ the weight_decay in the optimizer will be used. It should be noted that weight\
      \ decay can be a constant value or a Cell. It is a Cell only when dynamic weight\
      \ decay is applied. Dynamic weight decay is similar to dynamic learning rate,\
      \ users need to customize a weight decay schedule only with global step as input,\
      \ and during training, the optimizer calls the instance of WeightDecaySchedule\
      \ to get the weight decay value of current step. grad_centralization: Optional.\
      \ Must be Boolean. If \u201Cgrad_centralization\u201D is in the keys, the set\
      \ value will be used. If not, the grad_centralization is False by default. This\
      \ configuration only works on the convolution layer. order_params: Optional.\
      \ When parameters is grouped, this usually is used to maintain the order of\
      \ parameters that appeared in the network to improve performance. The value\
      \ should be parameters whose order will be followed in optimizer. If order_params\
      \ in the keys, other keys will be ignored and the element of \u2018order_params\u2019\
      \ must be in one group of params.  "
    dtype:
    - union[list[parameter], list[dict]]
    structure:
    - list
  update_slots:
    default: true
    descp: "update_slots (bool) \u2013 Whether the (h) will be updated. Default: True\
      \ ."
    dtype:
    - bool
  weight_decay:
    default: 0.0
    descp: "weight_decay (Union[float, int, Cell]) \u2013 Weight decay (L2 penalty).\
      \ Default: 0.0 .  float: The fixed weight decay value. Must be equal to or greater\
      \ than 0. int: The fixed weight decay value. Must be equal to or greater than\
      \ 0. It will be converted to float. Cell: Weight decay is dynamic. During training,\
      \ the optimizer calls the instance of the Cell with step as the input to get\
      \ the weight decay value of current step.  "
    dtype:
    - int
    - float
    - cell
    range: null
    shape: null
    structure:
    - single
descp: Implements the Adagrad algorithm. Adagrad is an online Learning and Stochastic
  Optimization.
inputs:
  optional:
  - accum
  - learning_rate
  - update_slots
  - loss_scale
  - weight_decay
  required:
  - params
