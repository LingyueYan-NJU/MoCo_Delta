api: 'mindspore.nn.thor(net, learning_rate, damping, momentum, weight_decay=0.0, loss_scale=1.0,
  batch_size=32, use_nesterov=False, decay_filter=lambda x: ..., split_indices=None,
  enable_clip_grad=False, frequency=100)'
constraints:
  batch_size:
    default: 32
    descp: "batch_size (int) \u2013 The size of a batch. Default: 32 ."
    dtype:
    - int
    range: null
    structure:
    - single
  damping:
    default: null
    descp: "damping (Tensor) \u2013 A value for the damping."
    dtype:
    - tensor
  decay_filter:
    default: null
    descp: "decay_filter (function) \u2013 A function to determine which layers the\
      \ weight decay applied to. And it only works when the weight_decay > 0. Default:\
      \ lambda x: x.name not in []"
    dtype:
    - function
  enable_clip_grad:
    default: false
    descp: "enable_clip_grad (bool) \u2013 Whether to clip the gradients. Default:\
      \ False ."
    dtype:
    - bool
  frequency:
    default: 100
    descp: "frequency (int) \u2013 The update interval of A/G and (A^{-1}/G^{-1}).\
      \ When frequency equals N (N is greater than 1), A/G and (A^{-1}/G^{-1}) will\
      \ be updated every N steps, and other steps will use the stale A/G and (A^{-1}/G^{-1})\
      \ to update weights. Default: 100 ."
    dtype:
    - int
    range: null
    structure:
    - single
  learning_rate:
    default: null
    descp: "learning_rate (Tensor) \u2013 A value for the learning rate."
    dtype:
    - tensor
  loss_scale:
    default: 1.0
    descp: "loss_scale (float) \u2013 A value for the loss scale. It must be greater\
      \ than 0.0. In general, use the default value. Default: 1.0 ."
    dtype:
    - float
  momentum:
    default: null
    descp: "momentum (float) \u2013 Hyper-parameter of type float, means momentum\
      \ for the moving average. It must be at least 0.0."
    dtype:
    - float
    shape: null
    structure:
    - single
  net:
    default: null
    descp: "net (Cell) \u2013 The training network."
    dtype:
    - cell
  split_indices:
    default: None
    descp: "split_indices (list) \u2013 Set allreduce fusion strategy by A/G layer\
      \ indices . Only works when distributed computing. ResNet50 as an example, there\
      \ are 54 layers of A/G respectively, when split_indices is set to [26, 53],\
      \ it means A/G is divided into two groups to allreduce,  one is 0~26 layer,\
      \ and the other is 27~53. Default: None ."
    dtype: null
    structure:
    - list
  use_nesterov:
    default: false
    descp: "use_nesterov (bool) \u2013 Enable Nesterov momentum. Default: False ."
    dtype:
    - bool
  weight_decay:
    default: 0.0
    descp: "weight_decay (int, float) \u2013 Weight decay (L2 penalty). It must be\
      \ equal to or greater than 0.0. Default: 0.0 ."
    dtype:
    - int
    - float
    range: null
    structure:
    - single
descp: "Updates gradients by second-order algorithm\u2013THOR."
inputs:
  optional:
  - weight_decay
  - loss_scale
  - batch_size
  - use_nesterov
  - decay_filter
  - split_indices
  - enable_clip_grad
  - frequency
  required:
  - net
  - learning_rate
  - damping
  - momentum
