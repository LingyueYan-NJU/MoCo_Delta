api: mindspore.nn.RReLU(lower=1 / 8, upper=1 / 3)
constraints:
  lower:
    default: 1/8
    descp: "lower (Union[int, float]) \u2013 Slope of the activation function at x\
      \ < 0. Default: 1 / 8 ."
    dtype:
    - int
    - float
    range: null
    structure:
    - single
  upper:
    default: 1/3
    descp: "upper (Union[int, float]) \u2013 Slope of the activation function at x\
      \ < 0. Default: 1 / 3 ."
    dtype:
    - int
    - float
    range: null
    structure:
    - single
descp: Randomized Leaky ReLU activation function.
inputs:
  optional:
  - lower
  - upper
  required: []
