api: mindspore.nn.AdamWeightDecay(params, learning_rate=0.001, beta1=0.9, beta2=0.999,
  eps=1e-06, weight_decay=0.0)
constraints:
  beta1:
    default: 0.9
    descp: "beta1 (float) \u2013 The exponential decay rate for the 1st moment estimations.\
      \ Default: 0.9 . Should be in range (0.0, 1.0)."
    dtype:
    - float
  beta2:
    default: 0.999
    descp: "beta2 (float) \u2013 The exponential decay rate for the 2nd moment estimations.\
      \ Default: 0.999 . Should be in range (0.0, 1.0)."
    dtype:
    - float
  eps:
    default: 1e-6
    descp: "eps (float) \u2013 Term added to the denominator to improve numerical\
      \ stability. Default: 1e-6 . Should be greater than 0."
    dtype:
    - float
  learning_rate:
    default: 1e-3
    descp: "learning_rate (Union[float, int, Tensor, Iterable, LearningRateSchedule])\
      \ \u2013 Default: 1e-3 .  float: The fixed learning rate value. Must be equal\
      \ to or greater than 0. int: The fixed learning rate value. Must be equal to\
      \ or greater than 0. It will be converted to float. Tensor: Its value should\
      \ be a scalar or a 1-D vector. For scalar, fixed learning rate will be applied.\
      \ For vector, learning rate is dynamic, then the i-th step will take the i-th\
      \ value as the learning rate. Iterable: Learning rate is dynamic. The i-th step\
      \ will take the i-th value as the learning rate. LearningRateSchedule: Learning\
      \ rate is dynamic. During training, the optimizer calls the instance of LearningRateSchedule\
      \ with step as the input to get the learning rate of current step.  "
    dtype:
    - int
    - float
    range: null
    shape: null
    structure:
    - single
  params:
    default: null
    descp: "params (Union[list[Parameter], list[dict]]) \u2013 Must be list of Parameter\
      \ or list of dict. When the params is a list of dict, the string \u201Cparams\u201D\
      , \u201Clr\u201D, \u201Cweight_decay\u201D, and \u201Corder_params\u201D are\
      \ the keys can be parsed.  params: Required. Parameters in current group. The\
      \ value must be a list of Parameter. lr: Optional. If \u201Clr\u201D in the\
      \ keys, the value of corresponding learning rate will be used. If not, the learning_rate\
      \ in optimizer will be used. Fixed and dynamic learning rate are supported.\
      \ weight_decay: Optional. If \u201Cweight_decay\u201D in the keys, the value\
      \ of corresponding weight decay will be used. If not, the weight_decay in the\
      \ optimizer will be used. It should be noted that weight decay can be a constant\
      \ value or a Cell. It is a Cell only when dynamic weight decay is applied. Dynamic\
      \ weight decay is similar to dynamic learning rate, users need to customize\
      \ a weight decay schedule only with global step as input, and during training,\
      \ the optimizer calls the instance of WeightDecaySchedule to get the weight\
      \ decay value of current step. order_params: Optional. When parameters is grouped,\
      \ this usually is used to maintain the order of parameters that appeared in\
      \ the network to improve performance. The value should be parameters whose order\
      \ will be followed in optimizer. If order_params in the keys, other keys will\
      \ be ignored and the element of \u2018order_params\u2019 must be in one group\
      \ of params.  "
    dtype:
    - union[list[parameter], list[dict]]
    structure:
    - list
  weight_decay:
    default: 0.0
    descp: "weight_decay (Union[float, int, Cell]) \u2013 Weight decay (L2 penalty).\
      \ Default: 0.0 .  float: The fixed weight decay value. Must be equal to or greater\
      \ than 0. int: The fixed weight decay value. Must be equal to or greater than\
      \ 0. It will be converted to float. Cell: Weight decay is dynamic. During training,\
      \ the optimizer calls the instance of the Cell with step as the input to get\
      \ the weight decay value of current step.  "
    dtype:
    - int
    - float
    - cell
    range: null
    shape: null
    structure:
    - single
descp: Implements the Adam algorithm with weight decay.
inputs:
  optional:
  - learning_rate
  - beta1
  - beta2
  - eps
  - weight_decay
  required:
  - params
