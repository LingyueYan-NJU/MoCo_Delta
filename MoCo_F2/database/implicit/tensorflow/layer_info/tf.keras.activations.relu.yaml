api: tf.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)
constraints:
  alpha:
    default: '0.0'
    descp: A `float` that governs the slope for values lower than the threshold.
    dtype:
    - float
  max_value:
    default: None
    descp: A `float` that sets the saturation threshold (the largest value the function
      will return).
    dtype:
    - float
  threshold:
    default: '0'
    descp: A `float` giving the threshold value of the activation function below which
      values will be damped or set to zero.
    dtype:
    - float
    - int
  x:
    descp: Input `tensor` or `variable`.
    tensor_t:
    - tf.tensor
descp: Applies the rectified linear unit activation function.
inputs:
  optional:
  - alpha
  - max_value
  - threshold
  required:
  - x
