api: tf.keras.layers.GRUCell(units, activation='tanh', recurrent_activation='sigmoid',
  use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',
  bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None,
  kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0,
  recurrent_dropout=0.0, reset_after=True, **kwargs)
constraints:
  activation:
    default: tanh
    descp: 'Activation function to use. Default: hyperbolic tangent (tanh). If you
      pass None, no activation is applied (ie. "linear" activation: a(x) = x).'
    dtype: tf.string
    enum:
    - tanh
    - None
  bias_constraint:
    default: None
    descp: 'Constraint function applied to the bias vector. Default: None.'
    dtype: tf.string
  bias_initializer:
    default: zeros
    descp: 'Initializer for the bias vector. Default: zeros.'
    dtype: tf.string
  bias_regularizer:
    default: None
    descp: 'Regularizer function applied to the bias vector. Default: None.'
    dtype: tf.string
  dropout:
    default: 0.0
    descp: 'Float between 0 and 1. Fraction of the units to drop for the linear transformation
      of the inputs. Default: 0.'
    dtype: float
  implementation:
    default: 2
    dtype: int
    range:
    - 0
    - 2048
    shape: 1
    structure: integer
  kernel_constraint:
    default: None
    descp: 'Constraint function applied to the kernel weights matrix. Default: None.'
    dtype: tf.string
  kernel_initializer:
    default: glorot_uniform
    descp: 'Initializer for the kernel weights matrix, used for the linear transformation
      of the inputs. Default: glorot_uniform.'
    dtype: tf.string
  kernel_regularizer:
    default: None
    descp: 'Regularizer function applied to the kernel weights matrix. Default: None.'
    dtype: tf.string
  recurrent_activation:
    default: sigmoid
    descp: 'Activation function to use for the recurrent step. Default: sigmoid (sigmoid).
      If you pass None, no activation is applied (ie. "linear" activation: a(x) =
      x).'
    dtype: tf.string
  recurrent_constraint:
    default: None
    descp: 'Constraint function applied to the recurrent_kernel weights matrix. Default:
      None.'
    dtype: tf.string
  recurrent_dropout:
    default: 0.0
    descp: 'Float between 0 and 1. Fraction of the units to drop for the linear transformation
      of the recurrent state. Default: 0.'
    dtype: float
  recurrent_initializer:
    default: orthogonal
    descp: 'Initializer for the recurrent_kernel weights matrix, used for the linear
      transformation of the recurrent state.  Default: orthogonal.'
    dtype: tf.string
  recurrent_regularizer:
    default: None
    descp: 'Regularizer function applied to the recurrent_kernel weights matrix. Default:
      None.'
    dtype: tf.string
  reset_after:
    default: true
    descp: GRU convention (whether to apply reset gate after or before matrix multiplication).
      False = "before", True = "after" (default and cuDNN compatible).
    dtype: tf.bool
  units:
    descp: Positive integer, dimensionality of the output space.
    dtype: int
    range:
    - 0
    - 2048
    shape: 1
    structure: integer
  use_bias:
    default: true
    descp: Boolean, (default True), whether the layer uses a bias vector.
    dtype: tf.bool
descp: Cell class for the GRU layer.
inputs:
  required:
  - units
  optional:
  - activation
  - bias_constraint
  - bias_initializer
  - bias_regularizer
  - dropout
  - implementation
  - kernel_constraint
  - kernel_initializer
  - kernel_regularizer
  - recurrent_activation
  - recurrent_constraint
  - recurrent_dropout
  - recurrent_initializer
  - recurrent_regularizer
  - reset_after
  - use_bias
