api: torch.nn.utils.clip_grad_value_(parameters, clip_value, foreach=None)
constraints:
  parameters:
    descp: an iterable of Tensors or a single Tensor that will have gradients normalized
    dtype:
    - iterable[tensor] or tensor
    - iterable[tensor] or tensor
  clip_value:
    descp: maximum allowed value of the gradients. The gradients are clipped in the
      range [-clip_value,clip_value]\left[\text{-clip\_value}, \text{clip\_value}\right][-clip_value,clip_value]
    dtype:
    - float
    structure:
    - single
    shape: null
  foreach:
    descp: use the faster foreach-based implementation If None, use the foreach implementation
      for CUDA and CPU native tensors and silently fall back to the slow implementation
      for other device types.
    default: None
    dtype:
    - bool
descp: Clips gradient of an iterable of parameters at specified value.
inputs:
  optional: []
  required:
  - parameters
  - clip_value
  - foreach
