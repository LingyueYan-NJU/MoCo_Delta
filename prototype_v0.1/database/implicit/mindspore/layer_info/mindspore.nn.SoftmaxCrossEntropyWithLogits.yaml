api: mindspore.nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='none')
constraints:
  reduction:
    default: none
    descp: "reduction (str) \u2013 Type of reduction to be applied to loss. The optional\
      \ values are \"mean\" , \"sum\" , and \"none\" . If \"none\" , do not perform\
      \ reduction. Default: \"none\" ."
    dtype:
    - str
    enum:
    - none
    - mean
    - sum
  sparse:
    default: false
    descp: "sparse (bool) \u2013 Specifies whether labels use sparse format or not.\
      \ Default: False ."
    dtype:
    - bool
descp: Computes softmax cross entropy between logits and labels.
inputs:
  optional:
  - sparse
  - reduction
  required: []
