api: mindspore.nn.optim_ex.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08,
  weight_decay=0.01, amsgrad=False, *, maximize=False)
constraints:
  amsgrad:
    default: false
    descp: "amsgrad (bool, optional) \u2013 whether to use the AMSGrad algorithm.\
      \ Default: False."
    dtype:
    - bool
  betas:
    default: (0.9, 0.999)
    descp: "betas (Tuple[float, float], optional) \u2013 The exponential decay rate\
      \ for the moment estimations. Default: (0.9, 0.999)."
    dtype:
    - float
    structure:
    - tuple
  eps:
    default: 1e-8
    descp: "eps (float, optional) \u2013 term added to the denominator to improve\
      \ numerical stability. Default: 1e-8."
    dtype:
    - float
  lr:
    default: 1e-3
    descp: "lr (Union[int, float, Tensor], optional) \u2013 learning rate. Default:\
      \ 1e-3."
    dtype:
    - int
    - float
    range: null
    structure:
    - single
  params:
    default: null
    descp: "params (Union[list(Parameter), list(dict)]) \u2013 list of parameters\
      \ to optimize or dicts defining parameter groups"
    dtype:
    - union[list(parameter), list(dict)]
    structure:
    - list
  weight_decay:
    default: 0
    descp: "weight_decay (float, optional) \u2013 weight decay (L2 penalty). Default:\
      \ 0."
    dtype:
    - float
descp: Implements Adam Weight Decay algorithm.
inputs:
  optional:
  - lr
  - betas
  - eps
  - weight_decay
  - amsgrad
  required:
  - params
