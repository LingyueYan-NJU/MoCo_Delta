api: mindspore.nn.LinearLR(optimizer, start_factor=1.0 / 3, end_factor=1.0, total_iters=5,
  last_epoch=- 1, verbose=False)
constraints:
  end_factor:
    default: 1.0
    descp: "end_factor (float, optional) \u2013 The number we multiply learning rate\
      \ at the end of linear changing process. Default: 1.0."
    dtype:
    - float
  last_epoch:
    default: -1
    descp: "last_epoch (int, optional) \u2013 The index of the last epoch. Default:\
      \ -1."
    dtype:
    - int
    range: null
    structure:
    - single
  optimizer:
    default: null
    descp: "optimizer (mindspore.nn.optim_ex.Optimizer) \u2013 Wrapped optimizer."
    dtype:
    - mindspore.nn.optim_ex.optimizer
  start_factor:
    default: 1/3
    descp: "start_factor (float, optional) \u2013 The number we multiply learning\
      \ rate in the first epoch. The multiplication factor changes towards end_factor\
      \ in the following epochs. Default: 1.0 /3."
    dtype:
    - float
  total_iters:
    default: 5
    descp: "total_iters (int, optional) \u2013 The number of iterations that multiplicative\
      \ factor reaches to 1. Default: 5."
    dtype:
    - int
    range: null
    structure:
    - single
  verbose:
    default: false
    descp: "verbose (bool, optional) \u2013 If True, prints a message to stdout for\
      \ each update. Default: False."
    dtype:
    - bool
descp: 'Decays the learning rate of each parameter group by linearly changing small
  multiplicative factor until the number of epoch reaches a pre-defined milestone:
  total_iters.'
inputs:
  optional:
  - start_factor
  - end_factor
  - total_iters
  - last_epoch
  - verbose
  required:
  - optimizer
