api: mindspore.nn.SGD(params, learning_rate=0.1, momentum=0.0, dampening=0.0, weight_decay=0.0,
  nesterov=False, loss_scale=1.0)
constraints:
  dampening:
    default: 0.0
    descp: "dampening (float) \u2013 A floating point value of dampening for momentum.\
      \ must be at least 0.0. Default: 0.0 ."
    dtype:
    - float
  learning_rate:
    default: 0.1
    descp: "learning_rate (Union[float, int, Tensor, Iterable, LearningRateSchedule])\
      \ \u2013 Default: 0.1 .  float: The fixed learning rate value. Must be equal\
      \ to or greater than 0. int: The fixed learning rate value. Must be equal to\
      \ or greater than 0. It will be converted to float. Tensor: Its value should\
      \ be a scalar or a 1-D vector. For scalar, fixed learning rate will be applied.\
      \ For vector, learning rate is dynamic, then the i-th step will take the i-th\
      \ value as the learning rate. Iterable: Learning rate is dynamic. The i-th step\
      \ will take the i-th value as the learning rate. LearningRateSchedule: Learning\
      \ rate is dynamic. During training, the optimizer calls the instance of LearningRateSchedule\
      \ with step as the input to get the learning rate of current step.  "
    dtype:
    - int
    - float
    range: null
    structure:
    - single
  loss_scale:
    default: 1.0
    descp: "loss_scale (float) \u2013 A floating point value for the loss scale, which\
      \ must be larger than 0.0. In general, use the default value. Only when FixedLossScaleManager\
      \ is used for training and the drop_overflow_update in FixedLossScaleManager\
      \ is set to False , then this value needs to be the same as the loss_scale in\
      \ FixedLossScaleManager. Refer to class mindspore.amp.FixedLossScaleManager\
      \ for more details. Default: 1.0 ."
    dtype:
    - float
  momentum:
    default: 0.0
    descp: "momentum (float) \u2013 A floating point value the momentum. must be at\
      \ least 0.0. Default: 0.0 ."
    dtype:
    - float
  nesterov:
    default: false
    descp: "nesterov (bool) \u2013 Enables the Nesterov momentum. If use nesterov,\
      \ momentum must be positive, and dampening must be equal to 0.0. Default: False\
      \ ."
    dtype:
    - bool
  params:
    default: null
    descp: "params (Union[list[Parameter], list[dict]]) \u2013 Must be list of Parameter\
      \ or list of dict. When the params is a list of dict, the string \u201Cparams\u201D\
      , \u201Clr\u201D, \u201Cgrad_centralization\u201D and \u201Corder_params\u201D\
      \ are the keys can be parsed.  params: Required. Parameters in current group.\
      \ The value must be a list of Parameter. lr: Optional. If \u201Clr\u201D in\
      \ the keys, the value of corresponding learning rate will be used. If not, the\
      \ learning_rate in optimizer will be used. Fixed and dynamic learning rate are\
      \ supported. weight_decay: Optional. If \u201Cweight_decay\u201D in the keys,\
      \ the value of corresponding weight decay will be used. If not, the weight_decay\
      \ in the optimizer will be used. It should be noted that weight decay must be\
      \ float, dynamic weight decay is currently not supported. grad_centralization:\
      \ Optional. Must be Boolean. If \u201Cgrad_centralization\u201D is in the keys,\
      \ the set value will be used. If not, the grad_centralization is False by default.\
      \ This configuration only works on the convolution layer. order_params: Optional.\
      \ When parameters is grouped, this usually is used to maintain the order of\
      \ parameters that appeared in the network to improve performance. The value\
      \ should be parameters whose order will be followed in optimizer. If order_params\
      \ in the keys, other keys will be ignored and the element of \u2018order_params\u2019\
      \ must be in one group of params.  "
    dtype:
    - union[list[parameter], list[dict]]
    structure:
    - list
  weight_decay:
    default: 0.0
    descp: "weight_decay (float) \u2013 Weight decay (L2 penalty). It must be equal\
      \ to or greater than 0. Default: 0.0 ."
    dtype:
    - float
descp: Implements stochastic gradient descent.
inputs:
  optional:
  - learning_rate
  - momentum
  - dampening
  - weight_decay
  - nesterov
  - loss_scale
  required:
  - params
