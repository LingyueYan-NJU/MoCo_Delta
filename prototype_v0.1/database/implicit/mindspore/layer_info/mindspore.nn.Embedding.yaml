api: mindspore.nn.Embedding(vocab_size, embedding_size, use_one_hot=False, embedding_table='normal',
  dtype=mstype.float32, padding_idx=None)
constraints:
  dtype:
    default: mstype.float32
    descp: "dtype (mindspore.dtype) \u2013 Data type of x. Default: mstype.float32\
      \ ."
    dtype:
    - mindspore.dtype
  embedding_size:
    default: null
    descp: "embedding_size (int) \u2013 The size of each embedding vector."
    dtype:
    - int
    range: null
    structure:
    - single
  embedding_table:
    default: normal
    descp: "embedding_table (Union[Tensor, str, Initializer, numbers.Number]) \u2013\
      \ Initializer for the embedding_table. Refer to class initializer for the values\
      \ of string when a string is specified. Default: 'normal' ."
    dtype:
    - str
    enum:
    - normal
  padding_idx:
    default: None
    descp: "padding_idx (int, None) \u2013 When the padding_idx encounters index,\
      \ the output embedding vector of this index will be initialized to zero. Default:\
      \ None . The feature is inactivated."
    dtype:
    - int
    - str
    enum:
    - None
    range: null
    structure:
    - single
  use_one_hot:
    default: false
    descp: "use_one_hot (bool) \u2013 Specifies whether to apply one_hot encoding\
      \ form. Default: False ."
    dtype:
    - bool
  vocab_size:
    default: null
    descp: "vocab_size (int) \u2013 Size of the dictionary of embeddings."
    dtype:
    - int
    range: null
    structure:
    - single
descp: A simple lookup table that stores embeddings of a fixed dictionary and size.
inputs:
  optional:
  - use_one_hot
  - embedding_table
  - dtype
  - padding_idx
  required:
  - vocab_size
  - embedding_size
