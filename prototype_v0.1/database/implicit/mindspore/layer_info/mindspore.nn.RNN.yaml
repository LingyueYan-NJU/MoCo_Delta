api: mindspore.nn.RNN(*args, **kwargs)
constraints:
  batch_first:
    default: false
    descp: "batch_first (bool) \u2013 Specifies whether the first dimension of input\
      \ x is batch_size. Default: False ."
    dtype:
    - bool
  bidirectional:
    default: false
    descp: "bidirectional (bool) \u2013 Specifies whether it is a bidirectional RNN,\
      \ num_directions=2 if bidirectional=True otherwise 1. Default: False ."
    dtype:
    - bool
  dropout:
    default: 0.0
    descp: "dropout (float) \u2013 If not 0.0, append Dropout layer on the outputs\
      \ of each RNN layer except the last layer. Default 0.0 . The range of dropout\
      \ is [0.0, 1.0)."
    dtype:
    - float
  has_bias:
    default: true
    descp: "has_bias (bool) \u2013 Whether the cell has bias b_ih and b_hh. Default:\
      \ True."
    dtype:
    - bool
  hidden_size:
    default: null
    descp: "hidden_size (int) \u2013 Number of features of hidden layer."
    dtype:
    - int
    range: null
    structure:
    - single
  input_size:
    default: null
    descp: "input_size (int) \u2013 Number of features of input."
    dtype:
    - int
    range: null
    structure:
    - single
  nonlinearity:
    default: tanh
    descp: "nonlinearity (str) \u2013 The non-linearity to use. Can be either 'tanh'\
      \ or 'relu'. Default: 'tanh'"
    dtype:
    - str
    enum:
    - tanh
    - relu
  num_layers:
    default: 1
    descp: "num_layers (int) \u2013 Number of layers of stacked RNN. Default: 1 ."
    dtype:
    - int
    range: null
    structure:
    - single
descp: Stacked Elman RNN layers.
inputs:
  optional:
  - num_layers
  - nonlinearity
  - has_bias
  - batch_first
  - dropout
  - bidirectional
  required:
  - input_size
  - hidden_size
