api: mindspore.nn.LeakyReLU(alpha=0.2)
constraints:
  alpha:
    default: 0.2
    descp: "alpha (Union[int, float]) \u2013 Slope of the activation function at x\
      \ < 0. Default: 0.2 ."
    dtype:
    - int
    - float
    range: null
    shape: null
    structure:
    - single
descp: Leaky ReLU activation function.
inputs:
  optional:
  - alpha
  required: []
