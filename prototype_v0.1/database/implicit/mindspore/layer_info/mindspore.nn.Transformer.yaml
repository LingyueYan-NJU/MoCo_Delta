api: 'mindspore.nn.Transformer(d_model: int = 512, nhead: int = 8, num_encoder_layers:
  int = 6, num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float
  = 0.1, activation: Union[str, Cell, callable] = ''relu'', custom_encoder: Optional[Cell]
  = None, custom_decoder: Optional[Cell] = None, layer_norm_eps: float = 1e-05, batch_first:
  bool = False, norm_first: bool = False)'
constraints:
  activation:
    default: relu
    descp: "activation (Union[str, callable, Cell]) \u2013 The activation function\
      \ of the intermediate layer, can be a string (\u201Crelu\u201D or \u201Cgelu\u201D\
      ), Cell instance (nn.ReLU() or nn.GELU()) or a callable (ops.relu or ops.gelu).\
      \ Default: \"relu\""
    dtype:
    - str
    enum:
    - relu
    - gelu
  batch_first:
    default: false
    descp: "batch_first (bool) \u2013 If batch_first = True, then the shape of input\
      \ and output tensors is ((batch, seq, feature)) , otherwise the shape is ((seq,\
      \ batch, feature)) . Default: False."
    dtype:
    - bool
  custom_decoder:
    default: None
    descp: "custom_decoder (Cell) \u2013 Custom decoder. Default: None."
    dtype:
    - cell
  custom_encoder:
    default: None
    descp: "custom_encoder (Cell) \u2013 Custom encoder. Default: None."
    dtype:
    - cell
  d_model:
    default: 512
    descp: "d_model (int) \u2013 The number of expected features in the inputs tensor.\
      \ Default: 512."
    dtype:
    - int
    range: null
    structure:
    - single
  dim_feedforward:
    default: 2048
    descp: "dim_feedforward (int) \u2013 The dimension of the feedforward layer. Default:\
      \ 2048."
    dtype:
    - int
    range: null
    structure:
    - single
  dropout:
    default: 0.1
    descp: "dropout (float) \u2013 The dropout value. Default: 0.1."
    dtype:
    - float
  layer_norm_eps:
    default: 1e-5
    descp: "layer_norm_eps (float) \u2013 the epsilion value in layer normalization\
      \ module. Default: 1e-5."
    dtype:
    - float
  nhead:
    default: 8
    descp: "nhead (int) \u2013 The number of heads in the MultiheadAttention modules.\
      \ Default: 8."
    dtype:
    - int
    range: null
    structure:
    - single
  norm_first:
    default: false
    descp: "norm_first (bool) \u2013 If norm_first = True, layer norm is done prior\
      \ to attention and feedforward operations, respectively. Default: False."
    dtype:
    - bool
  num_decoder_layers:
    default: 6
    descp: "num_decoder_layers (int) \u2013 The number of decoder-layers in the decoder.\
      \ Default: 6."
    dtype:
    - int
    range: null
    structure:
    - single
  num_encoder_layers:
    default: 6
    descp: "num_encoder_layers (int) \u2013 The number of encoder-layers in the encoder.\
      \ Default: 6."
    dtype:
    - int
    range: null
    structure:
    - single
descp: Transformer module including encoder and decoder.
inputs:
  optional:
  - d_model
  - nhead
  - num_encoder_layers
  - num_decoder_layers
  - dim_feedforward
  - dropout
  - activation
  - custom_encoder
  - custom_decoder
  - layer_norm_eps
  - batch_first
  - norm_first
  required: []
