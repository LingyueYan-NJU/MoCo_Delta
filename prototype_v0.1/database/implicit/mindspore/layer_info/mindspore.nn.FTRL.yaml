api: mindspore.nn.FTRL(params, initial_accum=0.1, learning_rate=0.001, lr_power=-
  0.5, l1=0.0, l2=0.0, use_locking=False, loss_scale=1.0, weight_decay=0.0)
constraints:
  initial_accum:
    default: 0.1
    descp: "initial_accum (float) \u2013 The starting value for accumulators m, must\
      \ be zero or positive values. Default: 0.1 ."
    dtype:
    - float
  l1:
    default: 0.0
    descp: "l1 (float) \u2013 l1 regularization strength, must be greater than or\
      \ equal to zero. Default: 0.0 ."
    dtype:
    - float
  l2:
    default: 0.0
    descp: "l2 (float) \u2013 l2 regularization strength, must be greater than or\
      \ equal to zero. Default: 0.0 ."
    dtype:
    - float
  learning_rate:
    default: 0.001
    descp: "learning_rate (float) \u2013 The learning rate value, must be zero or\
      \ positive, dynamic learning rate is currently not supported. Default: 0.001\
      \ ."
    dtype:
    - float
  loss_scale:
    default: 1.0
    descp: "loss_scale (float) \u2013 Value for the loss scale. It must be greater\
      \ than 0.0. In general, use the default value. Only when FixedLossScaleManager\
      \ is used for training and the drop_overflow_update in FixedLossScaleManager\
      \ is set to False , then this value needs to be the same as the loss_scale in\
      \ FixedLossScaleManager. Refer to class mindspore.amp.FixedLossScaleManager\
      \ for more details. Default: 1.0 ."
    dtype:
    - float
  lr_power:
    default: -0.5
    descp: "lr_power (float) \u2013 Learning rate power controls how the learning\
      \ rate decreases during training, must be less than or equal to zero. Use fixed\
      \ learning rate if lr_power is zero. Default: -0.5 ."
    dtype:
    - float
  params:
    default: null
    descp: "params (Union[list[Parameter], list[dict]]) \u2013 Must be list of Parameter\
      \ or list of dict. When the params is a list of dict, the string \u201Cparams\u201D\
      , \u201Cweight_decay\u201D, \u201Cgrad_centralization\u201D and \u201Corder_params\u201D\
      \ are the keys can be parsed.  params: Required. Parameters in current group.\
      \ The value must be a list of Parameter. lr: Using different learning rate by\
      \ grouping parameters is currently not supported. weight_decay: Optional. If\
      \ \u201Cweight_decay\u201D in the keys, the value of corresponding weight decay\
      \ will be used. If not, the weight_decay in the optimizer will be used. It should\
      \ be noted that weight decay can be a constant value or a Cell. It is a Cell\
      \ only when dynamic weight decay is applied. Dynamic weight decay is similar\
      \ to dynamic learning rate, users need to customize a weight decay schedule\
      \ only with global step as input, and during training, the optimizer calls the\
      \ instance of WeightDecaySchedule to get the weight decay value of current step.\
      \ grad_centralization: Optional. Must be Boolean. If \u201Cgrad_centralization\u201D\
      \ is in the keys, the set value will be used. If not, the grad_centralization\
      \ is False by default. This configuration only works on the convolution layer.\
      \ order_params: Optional. When parameters is grouped, this usually is used to\
      \ maintain the order of parameters that appeared in the network to improve performance.\
      \ The value should be parameters whose order will be followed in optimizer.\
      \ If order_params in the keys, other keys will be ignored and the element of\
      \ \u2018order_params\u2019 must be in one group of params.  "
    dtype:
    - union[list[parameter], list[dict]]
    structure:
    - list
  use_locking:
    default: false
    descp: "use_locking (bool) \u2013 If true, use locks for updating operation. Default:\
      \ False ."
    dtype:
    - bool
  weight_decay:
    default: 0.0
    descp: "weight_decay (Union[float, int, Cell]) \u2013 Weight decay (L2 penalty).\
      \ Default: 0.0 .  float: The fixed weight decay value. Must be equal to or greater\
      \ than 0. int: The fixed weight decay value. Must be equal to or greater than\
      \ 0. It will be converted to float. Cell: Weight decay is dynamic. During training,\
      \ the optimizer calls the instance of the Cell with step as the input to get\
      \ the weight decay value of current step.  "
    dtype:
    - int
    - float
    range: null
    structure:
    - single
descp: Implements the FTRL algorithm.
inputs:
  optional:
  - initial_accum
  - learning_rate
  - lr_power
  - l1
  - l2
  - use_locking
  - loss_scale
  - weight_decay
  required:
  - params
