api: torch.nn.parallel.DistributedDataParallel(class , module, device_ids=None, output_device=None,
  dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, find_unused_parameters=False,
  check_reduction=False, gradient_as_bucket_view=False, static_graph=False)
constraints:
  module:
    descp: module to be parallelized
    dtype:
    - module
  device_ids:
    descp: 'CUDA devices. 1) For single-device modules, device_ids can contain exactly
      one device id, which represents the only CUDA device where the input module
      corresponding to this process resides. Alternatively, device_ids can also be
      None. 2) For multi-device modules and CPU modules, device_ids must be None.
      When device_ids is None for both cases, both the input data for the forward
      pass and the actual module must be placed on the correct device. (default: None)CUDA
      devices. 1) For single-device modules, device_ids can contain exactly one device
      id, which represents the only CUDA device where the input module corresponding
      to this process resides. Alternatively, device_ids can also be None. 2) For
      multi-device modules and CPU modules, device_ids must be None.

      When device_ids is None for both cases, both the input data for the forward
      pass and the actual module must be placed on the correct device. (default: None)'
    dtype:
    - list of python:int or torch.device
    - list of python:int or torch.device
    structure:
    - list
  output_device:
    descp: 'Device location of output for single-device CUDA modules. For multi-device
      modules and CPU modules, it must be None, and the module itself dictates the
      output location. (default: device_ids[0] for single-device modules)'
    dtype:
    - int
    - int or torch.device
    structure:
    - single
    shape: null
    range: null
  broadcast_buffers:
    descp: 'Flag that enables syncing (broadcasting) buffers of the module at beginning
      of the forward function. (default: True)'
    dtype:
    - bool
  process_group:
    descp: 'The process group to be used for distributed data all-reduction. If None,
      the default process group, which is created by torch.distributed.init_process_group(),
      will be used. (default: None)'
  bucket_cap_mb:
    descp: 'DistributedDataParallel will bucket parameters into multiple buckets so
      that gradient reduction of each bucket can potentially overlap with backward
      computation. bucket_cap_mb controls the bucket size in MegaBytes (MB). (default:
      25)'
  find_unused_parameters:
    descp: "Traverse the autograd graph from all tensors contained in the return value\
      \ of the wrapped module\u2019s forward function. Parameters that don\u2019t\
      \ receive gradients as part of this graph are preemptively marked as being ready\
      \ to be reduced. In addition, parameters that may have been used in the wrapped\
      \ module\u2019s forward function but were not part of loss computation and thus\
      \ would also not receive gradients are preemptively marked as ready to be reduced.\
      \ (default: False)"
    dtype:
    - bool
  check_reduction:
    descp: This argument is deprecated.
  gradient_as_bucket_view:
    descp: When set to True, gradients will be views pointing to different offsets
      of allreduce communication buckets. This can reduce peak memory usage, where
      the saved memory size will be equal to the total gradients size. Moreover, it
      avoids the overhead of copying between gradients and allreduce communication
      buckets. When gradients are views, detach_() cannot be called on the gradients.
      If hitting such errors, please fix it by referring to the zero_grad() function
      in torch/optim/optimizer.py as a solution. Note that gradients will be views
      after first iteration, so the peak memory saving should be checked after first
      iteration.
    dtype:
    - bool
  static_graph:
    descp: 'When set to True, DDP knows the trained graph is static. Static graph
      means 1) The set of used and unused parameters will not change during the whole
      training loop; in this case, it does not matter whether users set find_unused_parameters
      = True or not. 2) How the graph is trained will not change during the whole
      training loop (meaning there is no control flow depending on iterations). When
      static_graph is set to be True, DDP will support cases that can not be supported
      in the past: 1) Reentrant backwards. 2) Activation checkpointing multiple times.
      3) Activation checkpointing when model has unused parameters. 4) There are model
      parameters that are outside of forward function. 5) Potentially improve performance
      when there are unused parameters, as DDP will not search graph in each iteration
      to detect unused parameters when static_graph is set to be True. To check whether
      you can set static_graph to be True, one way is to check ddp logging data at
      the end of your previous model training, if ddp_logging_data.get("can_set_static_graph")
      == True, mostly you can set static_graph = True as well.  Example::>>> model_DDP
      = torch.nn.parallel.DistributedDataParallel(model) >>> # Training loop >>> ...
      >>> ddp_logging_data = model_DDP._get_ddp_logging_data() >>> static_graph =
      ddp_logging_data.get("can_set_static_graph")When set to True, DDP knows the
      trained graph is static. Static graph means 1) The set of used and unused parameters
      will not change during the whole training loop; in this case, it does not matter
      whether users set find_unused_parameters = True or not. 2) How the graph is
      trained will not change during the whole training loop (meaning there is no
      control flow depending on iterations). When static_graph is set to be True,
      DDP will support cases that can not be supported in the past: 1) Reentrant backwards.
      2) Activation checkpointing multiple times. 3) Activation checkpointing when
      model has unused parameters. 4) There are model parameters that are outside
      of forward function. 5) Potentially improve performance when there are unused
      parameters, as DDP will not search graph in each iteration to detect unused
      parameters when static_graph is set to be True. To check whether you can set
      static_graph to be True, one way is to check ddp logging data at the end of
      your previous model training, if ddp_logging_data.get("can_set_static_graph")
      == True, mostly you can set static_graph = True as well.'
    dtype:
    - bool
descp: Implements distributed data parallelism that is based on torch.distributed
  package at the module level.
inputs:
  optional: []
  required:
  - module
  - device_ids
  - output_device
  - broadcast_buffers
  - process_group
  - bucket_cap_mb
  - find_unused_parameters
  - check_reduction
  - gradient_as_bucket_view
  - static_graph
