api: 'mindspore.nn.thor(net, learning_rate, damping, momentum, weight_decay=0.0, loss_scale=1.0,
  batch_size=32, use_nesterov=False, decay_filter=lambda x: ..., split_indices=None,
  enable_clip_grad=False, frequency=100)'
descp: Updates gradients by second-order algorithm–THOR.
constraints:
  net:
    descp: net (Cell) – The training network.
    default: null
    dtype:
    - cell
  learning_rate:
    descp: learning_rate (Tensor) – A value for the learning rate.
    default: null
    dtype:
    - tensor
  damping:
    descp: damping (Tensor) – A value for the damping.
    default: null
    dtype:
    - tensor
  momentum:
    descp: momentum (float) – Hyper-parameter of type float, means momentum for the
      moving average. It must be at least 0.0.
    default: null
    dtype:
    - float
    structure:
    - single
    shape: null
  weight_decay:
    descp: 'weight_decay (int, float) – Weight decay (L2 penalty). It must be equal
      to or greater than 0.0. Default: 0.0 .'
    default: 0.0
    dtype:
    - int
    - float
    structure:
    - single
    range: null
  loss_scale:
    descp: 'loss_scale (float) – A value for the loss scale. It must be greater than
      0.0. In general, use the default value. Default: 1.0 .'
    default: 1.0
    dtype:
    - float
  batch_size:
    descp: 'batch_size (int) – The size of a batch. Default: 32 .'
    default: 32
    dtype:
    - int
    structure:
    - single
    range: null
  use_nesterov:
    descp: 'use_nesterov (bool) – Enable Nesterov momentum. Default: False .'
    default: False
    dtype:
    - bool
  decay_filter:
    descp: 'decay_filter (function) – A function to determine which layers the weight
      decay applied to. And it only works when the weight_decay > 0. Default: lambda
      x: x.name not in []'
    default: null
    dtype:
    - function
  split_indices:
    descp: 'split_indices (list) – Set allreduce fusion strategy by A/G layer indices
      . Only works when distributed computing. ResNet50 as an example, there are 54
      layers of A/G respectively, when split_indices is set to [26, 53], it means
      A/G is divided into two groups to allreduce,  one is 0~26 layer, and the other
      is 27~53. Default: None .'
    default: None
    dtype: null
    structure:
    - list
  enable_clip_grad:
    descp: 'enable_clip_grad (bool) – Whether to clip the gradients. Default: False .'
    default: False
    dtype:
    - bool
  frequency:
    descp: 'frequency (int) – The update interval of A/G and (A^{-1}/G^{-1}). When
      frequency equals N (N is greater than 1), A/G and (A^{-1}/G^{-1}) will be updated
      every N steps, and other steps will use the stale A/G and (A^{-1}/G^{-1}) to
      update weights. Default: 100 .'
    default: 100
    dtype:
    - int
    structure:
    - single
    range: null
inputs:
  optional:
  - weight_decay
  - loss_scale
  - batch_size
  - use_nesterov
  - decay_filter
  - split_indices
  - enable_clip_grad
  - frequency
  required:
  - net
  - learning_rate
  - damping
  - momentum
