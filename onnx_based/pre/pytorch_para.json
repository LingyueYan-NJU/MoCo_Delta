{"alpha": [["torch.add", "type: Number. the multiplier for other."], ["torch.nn.CELU", "type: float. the \u03b1\\alpha\u03b1 value for the CELU formulation. Default: 1.0"], ["torch.nn.ELU", "type: float. the \u03b1\\alpha\u03b1 value for the ELU formulation. Default: 1.0"], ["torch.nn.LocalResponseNorm", "type: float. multiplicative factor. Default: 0.0001"]], "input": [["torch.add", "type: Tensor. the input tensor."], ["torch.ceil", "type: Tensor. the input tensor"], ["torch.erf", "type: Tensor. the input tensor."], ["torch.floor", "type: Tensor. the input tensor."], ["torch.gather", "type: Tensor. the source tensor"], ["torch.amax", "type: Tensor. the input tensor."], ["torch.amin", "type: Tensor. the input tensor."], ["torch.fmod", "type: Tensor. the dividend"], ["torch.pow", "type: Tensor. the input tensor."], ["torch.reciprocal", "type: Tensor. the input tensor."], ["torch.max", "type: Tensor. the input tensor."], ["torch.mean", "type: Tensor. the input tensor."], ["torch.min", "type: Tensor. the input tensor."], ["torch.prod", "type: Tensor. the input tensor."], ["torch.sum", "type: Tensor. the input tensor."], ["torch.nn.functional.relu", "type: Tensor. the input tensor."], ["torch.nn.ReLU", "type: Tensor. the input tensor."], ["torch.reshape", "type: Tensor. the tensor to be reshaped"], ["torch.round", "type: Tensor. the input tensor."]], "other": [["torch.add", "type: Tensor or Number. the tensor or number to add to input. "], ["torch.fmod", "type: Tensor or Scalar. the divisor "]], "out": [["torch.add", "type: Tensor. the output tensor. "], ["torch.cat", "type: Tensor. the output tensor."], ["torch.gather", "type: Tensor. the destination tensor "], ["torch.amax", "type: Tensor. the output tensor. "], ["torch.amin", "type: Tensor. the output tensor. "], ["torch.fmod", "type: Tensor. the output tensor."], ["torch.pow", "type: Tensor. the output tensor.  "], ["torch.max", "type: tuple. the result tuple of two output tensors (max, max_indices) "], ["torch.mean", "type: Tensor. the output tensor. "], ["torch.min", "type: tuple. the tuple of two output tensors (min, min_indices) "], ["torch.round", "type: Tensor. the output tensor."]], "ceil_mode": [["torch.nn.AvgPool1d", "type: bool. when True, will use ceil instead of floor to compute the output shape"], ["torch.nn.AvgPool2d", "type: bool. when True, will use ceil instead of floor to compute the output shape"], ["torch.nn.AvgPool3d", "type: bool. when True, will use ceil instead of floor to compute the output shape"], ["torch.nn.LPPool1d", "type: bool. when True, will use ceil instead of floor to compute the output shape "], ["torch.nn.LPPool2d", "type: bool. when True, will use ceil instead of floor to compute the output shape "], ["torch.nn.MaxPool1d", "type: bool. If True, will use ceil instead of floor to compute the output shape. This ensures that every element in the input tensor is covered by a sliding window. "], ["torch.nn.MaxPool2d", "type: bool. when True, will use ceil instead of floor to compute the output shape "], ["torch.nn.MaxPool3d", "type: bool. when True, will use ceil instead of floor to compute the output shape "]], "count_include_pad": [["torch.nn.AvgPool1d", "type: bool. when True, will include the zero-padding in the averaging calculation "], ["torch.nn.AvgPool2d", "type: bool. when True, will include the zero-padding in the averaging calculation"], ["torch.nn.AvgPool3d", "type: bool. when True, will include the zero-padding in the averaging calculation"]], "kernel_size": [["torch.nn.AvgPool1d", "type: Union[int, Tuple[int]]. the size of the window"], ["torch.nn.AvgPool2d", "type: Union[int, Tuple[int, int]]. the size of the window"], ["torch.nn.AvgPool3d", "type: Union[int, Tuple[int, int, int]]. the size of the window"], ["torch.nn.Conv1d", "type: int or tuple. Size of the convolving kernel"], ["torch.nn.Conv2d", "type: int or tuple. Size of the convolving kernel"], ["torch.nn.Conv3d", "type: int or tuple. Size of the convolving kernel"], ["torch.nn.ConvTranspose1d", "type: int or tuple. Size of the convolving kernel"], ["torch.nn.ConvTranspose2d", "type: int or tuple. Size of the convolving kernel"], ["torch.nn.ConvTranspose3d", "type: int or tuple. Size of the convolving kernel"], ["torch.nn.LPPool1d", "type: Union[int, Tuple[int]]. a single int, the size of the window"], ["torch.nn.LPPool2d", "type: Union[int, Tuple[int, int]]. the size of the window"], ["torch.nn.MaxPool1d", "type: Union[int, Tuple[int]]. The size of the sliding window, must be > 0."], ["torch.nn.MaxPool2d", "type: Union[int, Tuple[int, int]]. the size of the window to take a max over"], ["torch.nn.MaxPool3d", "type: Union[int, Tuple[int, int, int]]. the size of the window to take a max over"], ["torch.nn.MaxUnpool1d", "type: int or tuple. Size of the max pooling window."], ["torch.nn.MaxUnpool2d", "type: int or tuple. Size of the max pooling window."], ["torch.nn.MaxUnpool3d", "type: int or tuple. Size of the max pooling window."]], "padding": [["torch.nn.AvgPool1d", "type: Union[int, Tuple[int]]. implicit zero padding to be added on both sides"], ["torch.nn.AvgPool2d", "type: Union[int, Tuple[int, int]]. implicit zero padding to be added on both sides"], ["torch.nn.AvgPool3d", "type: Union[int, Tuple[int, int, int]]. implicit zero padding to be added on all three sides"], ["torch.nn.Conv1d", "type: int, tuple or str. Padding added to both sides of the input. Default: 0"], ["torch.nn.Conv2d", "type: int, tuple or str. Padding added to all four sides of the input. Default: 0"], ["torch.nn.Conv3d", "type: int, tuple or str. Padding added to all six sides of the input. Default: 0"], ["torch.nn.ConvTranspose1d", "type: int or tuple. dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of the input. Default: 0"], ["torch.nn.ConvTranspose2d", "type: int or tuple. dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0"], ["torch.nn.ConvTranspose3d", "type: int or tuple. dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0"], ["torch.nn.MaxPool1d", "type: Union[int, Tuple[int]]. Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2."], ["torch.nn.MaxPool2d", "type: Union[int, Tuple[int, int]]. Implicit negative infinity padding to be added on both sides"], ["torch.nn.MaxPool3d", "type: Union[int, Tuple[int, int, int]]. Implicit negative infinity padding to be added on all three sides"], ["torch.nn.MaxUnpool1d", "type: int or tuple. Padding that was added to the input "], ["torch.nn.MaxUnpool2d", "type: int or tuple. Padding that was added to the input "], ["torch.nn.MaxUnpool3d", "type: int or tuple. Padding that was added to the input "]], "stride": [["torch.nn.AvgPool1d", "type: Union[int, Tuple[int]]. the stride of the window. Default value is kernel_size"], ["torch.nn.AvgPool2d", "type: Union[int, Tuple[int, int]]. the stride of the window. Default value is kernel_size"], ["torch.nn.AvgPool3d", "type: Union[int, Tuple[int, int, int]]. the stride of the window. Default value is kernel_size"], ["torch.nn.Conv1d", "type: int or tuple. Stride of the convolution. Default: 1"], ["torch.nn.Conv2d", "type: int or tuple. Stride of the convolution. Default: 1"], ["torch.nn.Conv3d", "type: int or tuple. Stride of the convolution. Default: 1"], ["torch.nn.ConvTranspose1d", "type: int or tuple. Stride of the convolution. Default: 1"], ["torch.nn.ConvTranspose2d", "type: int or tuple. Stride of the convolution. Default: 1"], ["torch.nn.ConvTranspose3d", "type: int or tuple. Stride of the convolution. Default: 1"], ["torch.nn.LPPool1d", "type: Union[int, Tuple[int]]. a single int, the stride of the window. Default value is kernel_size"], ["torch.nn.LPPool2d", "type: Union[int, Tuple[int, int]]. the stride of the window. Default value is kernel_size"], ["torch.nn.MaxPool1d", "type: Union[int, Tuple[int]]. The stride of the sliding window, must be > 0. Default value is kernel_size."], ["torch.nn.MaxPool2d", "type: Union[int, Tuple[int, int]]. the stride of the window. Default value is kernel_size"], ["torch.nn.MaxPool3d", "type: Union[int, Tuple[int, int, int]]. the stride of the window. Default value is kernel_size"], ["torch.nn.MaxUnpool1d", "type: int or tuple. Stride of the max pooling window. It is set to kernel_size by default."], ["torch.nn.MaxUnpool2d", "type: int or tuple. Stride of the max pooling window. It is set to kernel_size by default."], ["torch.nn.MaxUnpool3d", "type: int or tuple. Stride of the max pooling window. It is set to kernel_size by default."]], "divisor_override": [["torch.nn.AvgPool2d", "type: Optional[int]. if specified, it will be used as divisor, otherwise size of the pooling region will be used. "], ["torch.nn.AvgPool3d", "type: Optional[int]. if specified, it will be used as divisor, otherwise kernel_size will be used "]], "affine": [["torch.nn.BatchNorm1d", "type: bool. a boolean value that when set to True, this module has learnable affine parameters. Default: True"], ["torch.nn.BatchNorm2d", "type: bool. a boolean value that when set to True, this module has learnable affine parameters. Default: True"], ["torch.nn.BatchNorm3d", "type: bool. a boolean value that when set to True, this module has learnable affine parameters. Default: True"], ["torch.nn.InstanceNorm1d", "type: bool. a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False."], ["torch.nn.InstanceNorm2d", "type: bool. a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False."], ["torch.nn.InstanceNorm3d", "type: bool. a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False."]], "eps": [["torch.nn.BatchNorm1d", "type: float. a value added to the denominator for numerical stability. Default: 1e-5"], ["torch.nn.BatchNorm2d", "type: float. a value added to the denominator for numerical stability. Default: 1e-5"], ["torch.nn.BatchNorm3d", "type: float. a value added to the denominator for numerical stability. Default: 1e-5"], ["torch.nn.InstanceNorm1d", "type: float. a value added to the denominator for numerical stability. Default: 1e-5"], ["torch.nn.InstanceNorm2d", "type: float. a value added to the denominator for numerical stability. Default: 1e-5"], ["torch.nn.InstanceNorm3d", "type: float. a value added to the denominator for numerical stability. Default: 1e-5"], ["torch.nn.LayerNorm", "type: float. a value added to the denominator for numerical stability. Default: 1e-5"]], "momentum": [["torch.nn.BatchNorm1d", "type: float. the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1"], ["torch.nn.BatchNorm2d", "type: float. the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1"], ["torch.nn.BatchNorm3d", "type: float. the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1"], ["torch.nn.InstanceNorm1d", "type: float. the value used for the running_mean and running_var computation. Default: 0.1"], ["torch.nn.InstanceNorm2d", "type: float. the value used for the running_mean and running_var computation. Default: 0.1"], ["torch.nn.InstanceNorm3d", "type: float. the value used for the running_mean and running_var computation. Default: 0.1"]], "num_features": [["torch.nn.BatchNorm1d", "type: int. number of features or channels CCC of the input"], ["torch.nn.BatchNorm2d", "type: int. CCC from an expected input of size (N,C,H,W)(N, C, H, W)(N,C,H,W)"], ["torch.nn.BatchNorm3d", "type: int. CCC from an expected input of size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)"], ["torch.nn.InstanceNorm1d", "type: int. number of features or channels CCC of the input"], ["torch.nn.InstanceNorm2d", "type: int. CCC from an expected input of size (N,C,H,W)(N, C, H, W)(N,C,H,W) or (C,H,W)(C, H, W)(C,H,W)"], ["torch.nn.InstanceNorm3d", "type: int. CCC from an expected input of size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) or (C,D,H,W)(C, D, H, W)(C,D,H,W)"]], "track_running_stats": [["torch.nn.BatchNorm1d", "type: bool. a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics. in both training and eval modes. Default: True "], ["torch.nn.BatchNorm2d", "type: bool. a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics. in both training and eval modes. Default: True "], ["torch.nn.BatchNorm3d", "type: bool. a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics. in both training and eval modes. Default: True "], ["torch.nn.InstanceNorm1d", "type: bool. a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False "], ["torch.nn.InstanceNorm2d", "type: bool. a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False "], ["torch.nn.InstanceNorm3d", "type: bool. a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False "]], "inplace": [["torch.nn.CELU", "type: bool. can optionally do the operation in-place. Default: False "], ["torch.nn.Dropout", "type: bool. If set to True, will do this operation in-place. Default: False "], ["torch.nn.Dropout1d", "type: bool. If set to True, will do this operation in-place "], ["torch.nn.Dropout2d", "type: bool. If set to True, will do this operation in-place "], ["torch.nn.Dropout3d", "type: bool. If set to True, will do this operation in-place "], ["torch.nn.functional.dropout", "type: bool. If set to True, will do this operation in-place. Default: False"], ["torch.nn.ELU", "type: bool. can optionally do the operation in-place. Default: False "], ["torch.nn.Hardsigmoid", "type: bool. can optionally do the operation in-place. Default: False"], ["torch.nn.Hardswish", "type: bool. can optionally do the operation in-place. Default: False"], ["torch.nn.LeakyReLU", "type: bool. can optionally do the operation in-place. Default: False "], ["torch.nn.functional.relu", "type: bool. can optionally do the operation in-place. Default: False"], ["torch.nn.SELU", "type: bool. can optionally do the operation in-place. Default: False"]], "dim": [["torch.cat", "type: int. the dimension over which the tensors are concatenated. "], ["torch.gather", "type: int. the axis along which to index"], ["torch.nn.LogSoftmax", "type: int.  dimension along which LogSoftmax will be computed."], ["torch.amax", "type: int or tuple of ints. the dimension or dimensions to reduce."], ["torch.amin", "type: int or tuple of ints. the dimension or dimensions to reduce."], ["torch.max", "type: int. the dimension to reduce."], ["torch.mean", "type: int or tuple of ints. the dimension or dimensions to reduce."], ["torch.min", "type: int. the dimension to reduce."], ["torch.prod", "type: int. the dimension to reduce."], ["torch.sum", "type: int or tuple of ints. the dimension or dimensions to reduce. If None, all dimensions are reduced."], ["torch.nn.Softmax", "type: int. A dimension along which Softmax will be computed (so every slice along dim will sum to 1)."]], "tensors": [["torch.cat", "type: sequence of Tensors. any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension."]], "bias": [["torch.nn.Conv1d", "type: bool. If True, adds a learnable bias to the output. Default: True "], ["torch.nn.Conv2d", "type: bool. If True, adds a learnable bias to the output. Default: True "], ["torch.nn.Conv3d", "type: bool. If True, adds a learnable bias to the output. Default: True "], ["torch.nn.ConvTranspose1d", "type: bool. If True, adds a learnable bias to the output. Default: True"], ["torch.nn.ConvTranspose2d", "type: bool. If True, adds a learnable bias to the output. Default: True"], ["torch.nn.ConvTranspose3d", "type: bool. If True, adds a learnable bias to the output. Default: True"], ["torch.nn.Linear", "type: bool. If set to False, the layer will not learn an additive bias. Default: True"], ["torch.nn.GRU", "type: bool. If False, then the layer does not use bias weights b_ih and b_hh. Default: True"], ["torch.nn.LayerNorm", "type: bool. If set to False, the layer will not learn an additive bias (only relevant if elementwise_affine is True). Default: True. "], ["torch.nn.LSTM", "type: bool. If False, then the layer does not use bias weights b_ih and b_hh. Default: True"], ["torch.nn.RNN", "type: bool. If False, then the layer does not use bias weights b_ih and b_hh. Default: True"]], "dilation": [["torch.nn.Conv1d", "type: int or tuple. Spacing between kernel elements. Default: 1"], ["torch.nn.Conv2d", "type: int or tuple. Spacing between kernel elements. Default: 1"], ["torch.nn.Conv3d", "type: int or tuple. Spacing between kernel elements. Default: 1"], ["torch.nn.ConvTranspose1d", "type: int or tuple. Spacing between kernel elements. Default: 1 "], ["torch.nn.ConvTranspose2d", "type: int or tuple. Spacing between kernel elements. Default: 1 "], ["torch.nn.ConvTranspose3d", "type: int or tuple. Spacing between kernel elements. Default: 1 "], ["torch.nn.MaxPool1d", "type: Union[int, Tuple[int]]. The stride between elements within a sliding window, must be > 0."], ["torch.nn.MaxPool2d", "type: Union[int, Tuple[int, int]]. a parameter that controls the stride of elements in the window"], ["torch.nn.MaxPool3d", "type: Union[int, Tuple[int, int, int]]. a parameter that controls the stride of elements in the window"]], "groups": [["torch.nn.Conv1d", "type: int. Number of blocked connections from input channels to output channels. Default: 1"], ["torch.nn.Conv2d", "type: int. Number of blocked connections from input channels to output channels. Default: 1"], ["torch.nn.Conv3d", "type: int. Number of blocked connections from input channels to output channels. Default: 1"], ["torch.nn.ConvTranspose1d", "type: int. Number of blocked connections from input channels to output channels. Default: 1"], ["torch.nn.ConvTranspose2d", "type: int. Number of blocked connections from input channels to output channels. Default: 1"], ["torch.nn.ConvTranspose3d", "type: int. Number of blocked connections from input channels to output channels. Default: 1"]], "in_channels": [["torch.nn.Conv1d", "type: int. Number of channels in the input image"], ["torch.nn.Conv2d", "type: int. Number of channels in the input image"], ["torch.nn.Conv3d", "type: int. Number of channels in the input image"], ["torch.nn.ConvTranspose1d", "type: int. Number of channels in the input image"], ["torch.nn.ConvTranspose2d", "type: int. Number of channels in the input image"], ["torch.nn.ConvTranspose3d", "type: int. Number of channels in the input image"]], "out_channels": [["torch.nn.Conv1d", "type: int. Number of channels produced by the convolution"], ["torch.nn.Conv2d", "type: int. Number of channels produced by the convolution"], ["torch.nn.Conv3d", "type: int. Number of channels produced by the convolution"], ["torch.nn.ConvTranspose1d", "type: int. Number of channels produced by the convolution"], ["torch.nn.ConvTranspose2d", "type: int. Number of channels produced by the convolution"], ["torch.nn.ConvTranspose3d", "type: int. Number of channels produced by the convolution"]], "padding_mode": [["torch.nn.Conv1d", "type: str. 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'"], ["torch.nn.Conv2d", "type: str. 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'"], ["torch.nn.Conv3d", "type: str. 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'"]], "output_padding": [["torch.nn.ConvTranspose1d", "type: int or tuple. Additional size added to one side of the output shape. Default: 0"], ["torch.nn.ConvTranspose2d", "type: int or tuple. Additional size added to one side of each dimension in the output shape. Default: 0"], ["torch.nn.ConvTranspose3d", "type: int or tuple. Additional size added to one side of each dimension in the output shape. Default: 0"]], "p": [["torch.nn.Dropout", "type: float. probability of an element to be zeroed. Default: 0.5"], ["torch.nn.Dropout1d", "type: float. probability of an element to be zero-ed."], ["torch.nn.Dropout2d", "type: float. probability of an element to be zero-ed."], ["torch.nn.Dropout3d", "type: float. probability of an element to be zeroed."], ["torch.nn.functional.dropout", "type: float. probability of an element to be zeroed. Default: 0.5"]], "training": [["torch.nn.functional.dropout", "type: bool. apply dropout if is True. Default: True"]], "equation": [["torch.einsum", "type: str. The subscripts for the Einstein summation."]], "operands": [["torch.einsum", "type: List[Tensor]. The tensors to compute the Einstein summation of. "]], "end_dim": [["torch.nn.Flatten", "type: int. last dim to flatten (default = -1). "]], "start_dim": [["torch.nn.Flatten", "type: int. first dim to flatten (default = 1)."]], "index": [["torch.gather", "type: LongTensor. the indices of elements to gather"]], "sparse_grad": [["torch.gather", "type: bool. If True, gradient w.r.t. input will be a sparse tensor."]], "approximate": [["torch.nn.GELU", "type: str. the gelu approximation algorithm to use: 'none' | 'tanh'. Default: 'none'"]], "in_features": [["torch.nn.Linear", "type: int.  size of each input sample"]], "out_features": [["torch.nn.Linear", "type: int. size of each output sample"]], "batch_first": [["torch.nn.GRU", "type: bool. If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False"], ["torch.nn.LSTM", "type: bool. If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False"], ["torch.nn.RNN", "type: bool. If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False"]], "bidirectional": [["torch.nn.GRU", "type: bool. If True, becomes a bidirectional GRU. Default: False"], ["torch.nn.LSTM", "type: bool. If True, becomes a bidirectional LSTM. Default: False"], ["torch.nn.RNN", "type: bool. If True, becomes a bidirectional RNN. Default: False"]], "dropout": [["torch.nn.GRU", "type: float. If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0"], ["torch.nn.LSTM", "type: float. If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0"], ["torch.nn.RNN", "type: float. If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0"]], "hidden_size": [["torch.nn.GRU", "type: int. The number of features in the hidden state h"], ["torch.nn.LSTM", "type: int. The number of features in the hidden state h"], ["torch.nn.RNN", "type: int. The number of features in the hidden state h"]], "input_size": [["torch.nn.GRU", "type: int. The number of expected features in the input x"], ["torch.nn.LSTM", "type: int. The number of expected features in the input x"], ["torch.nn.RNN", "type: int. The number of expected features in the input x"]], "num_layers": [["torch.nn.GRU", "type: int. Number of recurrent layers. Default: 1"], ["torch.nn.LSTM", "type: int. Number of recurrent layers. Default: 1"], ["torch.nn.RNN", "type: int. Number of recurrent layers. Default: 1"]], "elementwise_affine": [["torch.nn.LayerNorm", "type: bool. a boolean value that when set to True, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: True."]], "normalized_shape": [["torch.nn.LayerNorm", "type: int or list or torch.Size. input shape from an expected input of size. If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size."]], "negative_slope": [["torch.nn.LeakyReLU", "type: float. Controls the angle of the negative slope (which is used for negative input values). Default: 1e-2"]], "beta": [["torch.nn.LocalResponseNorm", "type: float. exponent. Default: 0.75"], ["torch.nn.Softplus", "type: int. the \u03b2\\beta\u03b2 value for the Softplus formulation. Default: 1"]], "k": [["torch.nn.LocalResponseNorm", "type: float. additive factor. Default: 1 "]], "size": [["torch.nn.LocalResponseNorm", "type: int. amount of neighbouring channels used for normalization"], ["torch.nn.Upsample", "type: int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]. output spatial sizes"]], "proj_size": [["torch.nn.LSTM", "type: int. If > 0, will use LSTM with projections of corresponding size. Default: 0"]], "keepdim": [["torch.amax", "type: bool. whether the output tensor has dim retained or not. "], ["torch.amin", "type: bool. whether the output tensor has dim retained or not. "], ["torch.max", "type: bool. whether the output tensor has dim retained or not. Default: False. "], ["torch.mean", "type: bool. whether the output tensor has dim retained or not. "], ["torch.min", "type: bool. whether the output tensor has dim retained or not. "], ["torch.prod", "type: bool. whether the output tensor has dim retained or not. "], ["torch.sum", "type: bool. whether the output tensor has dim retained or not. "]], "return_indices": [["torch.nn.MaxPool1d", "type: bool. If True, will return the argmax along with the max values. Useful for torch.nn.MaxUnpool1d later"], ["torch.nn.MaxPool2d", "type: bool. if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later"], ["torch.nn.MaxPool3d", "type: bool. if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool3d later"]], "exponent": [["torch.pow", "type: Tensor. the exponent tensor. "]], "self": [["torch.pow", "type: float. the scalar base value for the power operation"]], "init": [["torch.nn.PReLU", "type: float. the initial value of aaa. Default: 0.25 "]], "num_parameters": [["torch.nn.PReLU", "type: int. number of aaa to learn. Although it takes an int as input, there is only two values are legitimate: 1, or the number of channels at input. Default: 1"]], "dtype": [["torch.mean", "type: torch.dtype. the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None."], ["torch.prod", "type: torch.dtype. the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. "], ["torch.sum", "type: torch.dtype. the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. "]], "shape": [["torch.reshape", "type: tuple of int. the new shape "]], "nonlinearity": [["torch.nn.RNN", "type: str. The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'"]], "decimals": [["torch.round", "type: int. Number of decimal places to round to (default: 0). If decimals is negative, it specifies the number of positions to the left of the decimal point. "]], "threshold": [["torch.nn.Softplus", "type: int. values above this revert to a linear function. Default: 20 "]], "align_corners": [["torch.nn.Upsample", "type: bool. if True, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when mode is 'linear', 'bilinear', 'bicubic', or 'trilinear'. Default: False"]], "mode": [["torch.nn.Upsample", "type: str. the upsampling algorithm: one of 'nearest', 'linear', 'bilinear', 'bicubic' and 'trilinear'. Default: 'nearest'"]], "recompute_scale_factor": [["torch.nn.Upsample", "type: bool. recompute the scale_factor for use in the interpolation calculation. If recompute_scale_factor is True, then scale_factor must be passed in and scale_factor is used to compute the output size. The computed output size will be used to infer new scales for the interpolation. Note that when scale_factor is floating-point, it may differ from the recomputed scale_factor due to rounding and precision issues. If recompute_scale_factor is False, then size or scale_factor will be used directly for interpolation. "]], "scale_factor": [["torch.nn.Upsample", "type: float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float]. multiplier for spatial size. Has to match input size if it is a tuple."]]}