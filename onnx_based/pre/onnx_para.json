{"axis": [["Add", "type: INT. If set, defines the broadcast dimensions. See doc for details."], ["Concat", "type: INT. Which axis to concat on. A negative value means counting dimensions\nfrom the back. Accepted range is [-r, r-1] where r = rank(inputs)\u2026"], ["Div", "type: INT. If set, defines the broadcast dimensions. See doc for details."], ["Flatten", "type: INT. Indicate up to which input dimensions (exclusive) should be\nflattened to the outer dimension of the output. The value for axis\nmust be in the range [-r, r], where r is the rank of the input\ntensor. Negative value means counting dimensions from the back. When\naxis = 0, the shape of the output tensor is (1, (d_0 X d_1 \u2026 d_n),\nwhere the shape of the input tensor is (d_0, d_1, \u2026 d_n)."], ["Gather", "type: INT. Which axis to gather on. Negative value means counting dimensions\nfrom the back. Accepted range is [-r, r-1] where r = rank(data)."], ["LayerNormalization", "type: INT. The first normalization dimension. If rank(X) is r, axis\u2019 allowed\nrange is [-r, r). Negative value means counting dimensions from the\nback."], ["LogSoftmax", "type: INT.  Describes the dimension LogSoftmax will be performed on. Negative\nvalue means counting dimensions from the back. Accepted range is\n[-r, r-1] where r = rank(input)."], ["Mul", "type: INT. If set, defines the broadcast dimensions. See doc for details."], ["Pow", "type: INT. If set, defines the broadcast dimensions. See doc for details."], ["Softmax", "type: INT.  Describes the dimension Softmax will be performed on. Negative\nvalue means counting dimensions from the back. Accepted range is\n[-r, r-1] where r = rank(input)."], ["Sub", "type: INT. If set, defines the broadcast dimensions. See doc for details."]], "broadcast": [["Add", "type: INT. Pass 1 to enable broadcasting"], ["Div", "type: INT. Pass 1 to enable broadcasting"], ["Mul", "type: INT. Pass 1 to enable broadcasting"], ["Pow", "type: INT. Pass 1 to enable broadcasting"], ["Sub", "type: INT. Pass 1 to enable broadcasting"]], "auto_pad": [["AveragePool", "type: STRING. auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID.\nWhere default value is NOTSET, which means explicit padding is used.\nSAME_UPPER or SAME_LOWER mean pad the input so that output_shape[i] = ceil(input_shape[i] / strides[i]) for each axis i. The padding\nis split between the two sides equally or almost equally (depending\non whether it is even or odd). In case the padding is an odd number,\nthe extra padding is added at the end for SAME_UPPER and at the\nbeginning for SAME_LOWER."], ["Conv", "type: STRING. auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID.\nWhere default value is NOTSET, which means explicit padding is used.\nSAME_UPPER or SAME_LOWER mean pad the input so that output_shape[i] = ceil(input_shape[i] / strides[i]) for each axis i. The padding\nis split between the two sides equally or almost equally (depending\non whether it is even or odd). In case the padding is an odd number,\nthe extra padding is added at the end for SAME_UPPER and at the\nbeginning for SAME_LOWER."], ["ConvTranspose", "type: STRING. auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID.\nWhere default value is NOTSET, which means explicit padding is used.\nSAME_UPPER or SAME_LOWER mean pad the input so that output_shape[i] = input_shape[i] * strides[i] for each axis i. The padding is\nsplit between the two sides equally or almost equally (depending on\nwhether it is even or odd). In case the padding is an odd number,\nthe extra padding is added at the end for SAME_UPPER and at the\nbeginning for SAME_LOWER."], ["LpPool", "type: STRING. auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID.\nWhere default value is NOTSET, which means explicit padding is used.\nSAME_UPPER or SAME_LOWER mean pad the input so that output_shape[i] = ceil(input_shape[i] / strides[i]) for each axis i. The padding\nis split between the two sides equally or almost equally (depending\non whether it is even or odd). In case the padding is an odd number,\nthe extra padding is added at the end for SAME_UPPER and at the\nbeginning for SAME_LOWER."], ["MaxPool", "type: STRING. auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID.\nWhere default value is NOTSET, which means explicit padding is used.\nSAME_UPPER or SAME_LOWER mean pad the input so that output_shape[i] = ceil(input_shape[i] / strides[i]) for each axis i. The padding\nis split between the two sides equally or almost equally (depending\non whether it is even or odd). In case the padding is an odd number,\nthe extra padding is added at the end for SAME_UPPER and at the\nbeginning for SAME_LOWER."]], "ceil_mode": [["AveragePool", "type: INT. Whether to use ceil or floor (default) to compute the output shape."], ["LpPool", "type: INT. Whether to use ceil or floor (default) to compute the output shape."], ["MaxPool", "type: INT. Whether to use ceil or floor (default) to compute the output shape."]], "count_include_pad": [["AveragePool", "type: INT. Whether include pad pixels when calculating values for the edges.\nDefault is 0, doesn\u2019t count include pad."]], "dilations": [["AveragePool", "type: INTS. Dilation value along each spatial axis of filter. If not present,\nthe dilation defaults to 1 along each spatial axis."], ["Conv", "type: INTS. dilation value along each spatial axis of the filter. If not\npresent, the dilation defaults is 1 along each spatial axis."], ["ConvTranspose", "type: INTS. dilation value along each spatial axis of the filter. If not\npresent, the dilation defaults to 1 along each spatial axis."], ["LpPool", "type: INTS. dilation value along each spatial axis of the filter. If not\npresent, the dilation defaults is 1 along each spatial axis."], ["MaxPool", "type: INTS. Dilation value along each spatial axis of filter. If not present,\nthe dilation defaults to 1 along each spatial axis."]], "kernel_shape": [["AveragePool", "type: INTS. The size of the kernel along each axis."], ["Conv", "type: INTS. The shape of the convolution kernel. If not present, should be\ninferred from input W."], ["ConvTranspose", "type: INTS. The shape of the convolution kernel. If not present, should be\ninferred from input W."], ["LpPool", "type: INTS. The size of the kernel along each axis."], ["MaxPool", "type: INTS. The size of the kernel along each axis."], ["MaxUnpool", "type: INTS. The size of the kernel along each axis."]], "pads": [["AveragePool", "type: INTS. Padding for the beginning and ending along each spatial axis, it can\ntake any value greater than or equal to 0. The value represent the\nnumber of pixels added to the beginning and end part of the\ncorresponding axis. pads format should be as follow [x1_begin,\nx2_begin\u2026x1_end, x2_end,\u2026], where xi_begin the number of pixels\nadded at the beginning of axis i and xi_end, the number of pixels\nadded at the end of axis i. This attribute cannot be used\nsimultaneously with auto_pad attribute. If not present, the padding\ndefaults to 0 along start and end of each spatial axis."], ["Conv", "type: INTS. Padding for the beginning and ending along each spatial axis, it can\ntake any value greater than or equal to 0. The value represent the\nnumber of pixels added to the beginning and end part of the\ncorresponding axis. pads format should be as follow [x1_begin,\nx2_begin\u2026x1_end, x2_end,\u2026], where xi_begin the number of pixels\nadded at the beginning of axis i and xi_end, the number of pixels\nadded at the end of axis i. This attribute cannot be used\nsimultaneously with auto_pad attribute. If not present, the padding\ndefaults to 0 along start and end of each spatial axis."], ["ConvTranspose", "type: INTS. Padding for the beginning and ending along each spatial axis, it can\ntake any value greater than or equal to 0. The value represent the\nnumber of pixels added to the beginning and end part of the\ncorresponding axis. pads format should be as follow [x1_begin,\nx2_begin\u2026x1_end, x2_end,\u2026], where xi_begin the number of pixels\nadded at the beginning of axis i and xi_end, the number of pixels\nadded at the end of axis i. This attribute cannot be used\nsimultaneously with auto_pad attribute. If not present, the padding\ndefaults to 0 along start and end of each spatial axis."], ["LpPool", "type: INTS. Padding for the beginning and ending along each spatial axis, it can\ntake any value greater than or equal to 0. The value represent the\nnumber of pixels added to the beginning and end part of the\ncorresponding axis. pads format should be as follow [x1_begin,\nx2_begin\u2026x1_end, x2_end,\u2026], where xi_begin the number of pixels\nadded at the beginning of axis i and xi_end, the number of pixels\nadded at the end of axis i. This attribute cannot be used\nsimultaneously with auto_pad attribute. If not present, the padding\ndefaults to 0 along start and end of each spatial axis."], ["MaxPool", "type: INTS. Padding for the beginning and ending along each spatial axis, it can\ntake any value greater than or equal to 0. The value represent the\nnumber of pixels added to the beginning and end part of the\ncorresponding axis. pads format should be as follow [x1_begin,\nx2_begin\u2026x1_end, x2_end,\u2026], where xi_begin the number of pixels\nadded at the beginning of axis i and xi_end, the number of pixels\nadded at the end of axis i. This attribute cannot be used\nsimultaneously with auto_pad attribute. If not present, the padding\ndefaults to 0 along start and end of each spatial axis."], ["MaxUnpool", "type: INTS. Padding for the beginning and ending along each spatial axis, it can\ntake any value greater than or equal to 0. The value represent the\nnumber of pixels added to the beginning and end part of the\ncorresponding axis. pads format should be as follow [x1_begin,\nx2_begin\u2026x1_end, x2_end,\u2026], where xi_begin the number of pixels\nadded at the beginning of axis i and xi_end, the number of pixels\nadded at the end of axis i. This attribute cannot be used\nsimultaneously with auto_pad attribute. If not present, the padding\ndefaults to 0 along start and end of each spatial axis."]], "strides": [["AveragePool", "type: INTS. Stride along each spatial axis. If not present, the stride defaults\nto 1 along each spatial axis."], ["Conv", "type: INTS. Stride along each spatial axis. If not present, the stride defaults\nis 1 along each spatial axis."], ["ConvTranspose", "type: INTS. Stride along each spatial axis. If not present, the stride defaults\nto 1 along each spatial axis."], ["LpPool", "type: INTS. Stride along each spatial axis. If not present, the stride defaults\nto 1 along each spatial axis."], ["MaxPool", "type: INTS. Stride along each spatial axis. If not present, the stride defaults\nto 1 along each spatial axis."], ["MaxUnpool", "type: INTS. Stride along each spatial axis. If not present, the stride defaults\nto 1 along each spatial axis."]], "epsilon": [["BatchNormalization", "type: FLOAT. The epsilon value to use to avoid division by zero."], ["InstanceNormalization", "type: FLOAT. The epsilon value to use to avoid division by zero."], ["LayerNormalization", "type: FLOAT. The epsilon value to use to avoid division by zero."]], "momentum": [["BatchNormalization", "type: FLOAT. Factor used in computing the running mean and variance.e.g.,\nrunning_mean = running_mean * momentum + mean * (1 - momentum)."]], "training_mode": [["BatchNormalization", "type: INT. If set to true, it indicates BatchNormalization is being used for\ntraining, and outputs 1, 2, 3, and 4 would be populated."]], "consumed_inputs": [["Ceil", "type: INTS. legacy optimization attribute."], ["Floor", "type: INTS. legacy optimization attribute."], ["Max", "type: INTS. legacy optimization attribute."], ["Min", "type: INTS. legacy optimization attribute."], ["Neg", "type: INTS. legacy optimization attribute."], ["PRelu", "type: INTS. legacy optimization attribute."], ["Reciprocal", "type: INTS. legacy optimization attribute."], ["Relu", "type: INTS. legacy optimization attribute."], ["Sigmoid", "type: INTS. legacy optimization attribute."], ["Tanh", "type: INTS. legacy optimization attribute."]], "alpha": [["Celu", "type: FLOAT. The Alpha value in Celu formula which control the shape of the unit.\nThe default value is 1.0."], ["Elu", "type: FLOAT. Coefficient of ELU."], ["Gemm", "type: FLOAT. Scalar multiplier for the product of input tensors A * B."], ["HardSigmoid", "type: FLOAT. Value of alpha."], ["LeakyRelu", "type: FLOAT. Coefficient of leakage."], ["LRN", "type: FLOAT. Scaling parameter."], ["Selu", "type: FLOAT. Coefficient of SELU default to 1.67326319217681884765625 (i.e.,\nfloat32 approximation of 1.6732632423543772848170429916717)."], ["ThresholdedRelu", "type: FLOAT. Threshold value"]], "sparse_value": [["Constant", "type: SPARSE_TENSOR. The value for the elements of the output tensor in sparse format."]], "value": [["Constant", "type: TENSOR. The value for the elements of the output tensor."]], "value_float": [["Constant", "type: FLOAT. The value for the sole element for the scalar, float32, output\ntensor."]], "value_floats": [["Constant", "type: FLOATS. The values for the elements for the 1D, float32, output tensor."]], "value_int": [["Constant", "type: INT. The value for the sole element for the scalar, int64, output tensor."]], "value_ints": [["Constant", "type: INTS. The values for the elements for the 1D, int64, output tensor."]], "value_string": [["Constant", "type: STRING. The value for the sole element for the scalar, UTF-8 string, output\ntensor."]], "value_strings": [["Constant", "type: STRINGS. The values for the elements for the 1D, UTF-8 string, output tensor."]], "group": [["Conv", "type: INT. number of groups input channels and output channels are divided\ninto."], ["ConvTranspose", "type: INT. number of groups input channels and output channels are divided\ninto."]], "output_padding": [["ConvTranspose", "type: INTS. Additional elements added to the side with higher coordinate indices\nin the output. Each padding value in \u201coutput_padding\u201d must be less\nthan the corresponding stride/dilation dimension. By default, this\nattribute is a zero vector. Note that this attribute doesn\u2019t\ndirectly affect the computed output values. It only controls the\nselection of the computed values, so changing this attribute only\nadds or removes output elements. If \u201coutput_shape\u201d is explicitly\nprovided, \u201coutput_padding\u201d does not contribute additional size to\n\u201coutput_shape\u201d but participates in the computation of the needed\npadding amount. This is also called adjs or adjustment in some\nframeworks."]], "output_shape": [["ConvTranspose", "type: INTS. The shape of the output can be explicitly set which will cause pads\nvalues to be auto generated. If output_shape is specified pads\nvalues are ignored. See doc for details for equations to generate\npads. Note that the output_shape attribute value should not include\ndimensions for batch size and channels, which are automatically\ninferred."]], "seed": [["Dropout", "type: INT. (Optional) Seed to the random generator, if not specified we will\nauto generate one."]], "equation": [["Einsum", "type: STRING. Einsum expression string."]], "approximate": [["Gelu", "type: STRING. Gelu approximation algorithm: \"tanh\", \"none\"(default).\"none\":\ndo not use approximation.\"tanh\": use tanh approximation."]], "beta": [["Gemm", "type: FLOAT. Scalar multiplier for input tensor C."], ["HardSigmoid", "type: FLOAT. Value of beta."], ["LRN", "type: FLOAT. The exponent."]], "transA": [["Gemm", "type: INT. Whether A should be transposed"]], "transB": [["Gemm", "type: INT. Whether B should be transposed"]], "activations": [["GRU", "type: STRINGS. A list of 2 (or 4 if bidirectional) activation functions for update,\nreset, and hidden gates. The activation functions must be one of the\nactivation functions specified above. Optional: See the equations\nfor default if not specified."], ["LSTM", "type: STRINGS. A list of 3 (or 6 if bidirectional) activation functions for input,\noutput, forget, cell, and hidden. The activation functions must be\none of the activation functions specified above. Optional: See the\nequations for default if not specified."], ["RNN", "type: STRINGS. One (or two if bidirectional) activation function for input gate.\nThe activation function must be one of the activation functions\nspecified above. Optional: Default Tanh if not specified."]], "activation_alpha": [["GRU", "type: FLOATS. Optional scaling values used by some activation functions. The\nvalues are consumed in the order of activation functions, for\nexample (f, g, h) in LSTM. Default values are the same as of\ncorresponding ONNX operators.For example with LeakyRelu, the default\nalpha is 0.01."], ["LSTM", "type: FLOATS. Optional scaling values used by some activation functions. The\nvalues are consumed in the order of activation functions, for\nexample (f, g, h) in LSTM. Default values are the same as of\ncorresponding ONNX operators.For example with LeakyRelu, the default\nalpha is 0.01."], ["RNN", "type: FLOATS. Optional scaling values used by some activation functions. The\nvalues are consumed in the order of activation functions, for\nexample (f, g, h) in LSTM. Default values are the same as of\ncorresponding ONNX operators.For example with LeakyRelu, the default\nalpha is 0.01."]], "activation_beta": [["GRU", "type: FLOATS. Optional scaling values used by some activation functions. The\nvalues are consumed in the order of activation functions, for\nexample (f, g, h) in LSTM. Default values are the same as of\ncorresponding ONNX operators."], ["LSTM", "type: FLOATS. Optional scaling values used by some activation functions. The\nvalues are consumed in the order of activation functions, for\nexample (f, g, h) in LSTM. Default values are the same as of\ncorresponding ONNX operators."], ["RNN", "type: FLOATS. Optional scaling values used by some activation functions. The\nvalues are consumed in the order of activation functions, for\nexample (f, g, h) in LSTM. Default values are the same as of\ncorresponding ONNX operators."]], "clip": [["GRU", "type: FLOAT. Cell clip threshold. Clipping bounds the elements of a tensor in the\nrange of [-threshold, +threshold] and is applied to the input of\nactivations. No clip if not specified."], ["LSTM", "type: FLOAT. Cell clip threshold. Clipping bounds the elements of a tensor in the\nrange of [-threshold, +threshold] and is applied to the input of\nactivations. No clip if not specified."], ["RNN", "type: FLOAT. Cell clip threshold. Clipping bounds the elements of a tensor in the\nrange of [-threshold, +threshold] and is applied to the input of\nactivations. No clip if not specified."]], "direction": [["GRU", "type: STRING. Specify if the RNN is forward, reverse, or bidirectional. Must be\none of forward (default), reverse, or bidirectional."], ["LSTM", "type: STRING. Specify if the RNN is forward, reverse, or bidirectional. Must be\none of forward (default), reverse, or bidirectional."], ["RNN", "type: STRING. Specify if the RNN is forward, reverse, or bidirectional. Must be\none of forward (default), reverse, or bidirectional."]], "hidden_size": [["GRU", "type: INT. Number of neurons in the hidden layer"], ["LSTM", "type: INT. Number of neurons in the hidden layer"], ["RNN", "type: INT. Number of neurons in the hidden layer"]], "layout": [["GRU", "type: INT. The shape format of inputs X, initial_h and outputs Y, Y_h. If 0,\nthe following shapes are expected: X.shape = [seq_length,\nbatch_size, input_size], Y.shape = [seq_length, num_directions,\nbatch_size, hidden_size], initial_h.shape = Y_h.shape =\n[num_directions, batch_size, hidden_size]. If 1, the following\nshapes are expected: X.shape = [batch_size, seq_length, input_size],\nY.shape = [batch_size, seq_length, num_directions, hidden_size],\ninitial_h.shape = Y_h.shape = [batch_size, num_directions,\nhidden_size]."], ["LSTM", "type: INT. The shape format of inputs X, initial_h, initial_c and outputs Y,\nY_h, Y_c. If 0, the following shapes are expected: X.shape =\n[seq_length, batch_size, input_size], Y.shape = [seq_length,\nnum_directions, batch_size, hidden_size], initial_h.shape =\nY_h.shape = initial_c.shape = Y_c.shape = [num_directions,\nbatch_size, hidden_size]. If 1, the following shapes are expected:\nX.shape = [batch_size, seq_length, input_size], Y.shape =\n[batch_size, seq_length, num_directions, hidden_size],\ninitial_h.shape = Y_h.shape = initial_c.shape = Y_c.shape =\n[batch_size, num_directions, hidden_size]."], ["RNN", "type: INT. The shape format of inputs X, initial_h and outputs Y, Y_h. If 0,\nthe following shapes are expected: X.shape = [seq_length,\nbatch_size, input_size], Y.shape = [seq_length, num_directions,\nbatch_size, hidden_size], initial_h.shape = Y_h.shape =\n[num_directions, batch_size, hidden_size]. If 1, the following\nshapes are expected: X.shape = [batch_size, seq_length, input_size],\nY.shape = [batch_size, seq_length, num_directions, hidden_size],\ninitial_h.shape = Y_h.shape = [batch_size, num_directions,\nhidden_size]."]], "linear_before_reset": [["GRU", "type: INT. When computing the output of the hidden gate, apply the linear\ntransformation before multiplying by the output of the reset gate."]], "stash_type": [["LayerNormalization", "type: INT. Type of Mean and InvStdDev. This also specifies stage one\u2019s\ncomputation precision."]], "p": [["LpPool", "type: INT. p value of the Lp norm used to pool over the input data."]], "bias": [["LRN", "type: FLOAT. "]], "size": [["LRN", "type: INT. The number of channels to sum over"]], "input_forget": [["LSTM", "type: INT. Couple the input and forget gates if 1."]], "storage_order": [["MaxPool", "type: INT. The storage order of the tensor. 0 is row major, and 1 is column\nmajor. This attribute is used only to convert an n-tuple index value\ninto a single integer value for producing the second output."]], "fmod": [["Mod", "type: INT. Whether the operator should behave like fmod (default=0 meaning it\nwill do integer mods); Set this to 1 to force fmod treatment"]], "keepdims": [["ReduceMax", "type: INT. Keep the reduced dimension or not, default 1 means keep reduced\ndimension."], ["ReduceMean", "type: INT. Keep the reduced dimension or not, default 1 means keep reduced\ndimension."], ["ReduceMin", "type: INT. Keep the reduced dimension or not, default 1 means keep reduced\ndimension."], ["ReduceProd", "type: INT. Keep the reduced dimension or not, default 1 means keep reduced\ndimension."], ["ReduceSum", "type: INT. Keep the reduced dimension or not, default 1 means keep reduced\ndimension."]], "noop_with_empty_axes": [["ReduceMax", "type: INT. Defines behavior if \u2018axes\u2019 is empty. Default behavior with \u2018false\u2019\nis to reduce all axes. When axes is empty and this attribute is set\nto true, input tensor will not be reduced,and the output tensor\nwould be equivalent to input tensor."], ["ReduceMean", "type: INT. Defines behavior if \u2018axes\u2019 is empty. Default behavior with \u2018false\u2019\nis to reduce all axes. When axes is empty and this attribute is set\nto true, input tensor will not be reduced,and the output tensor\nwould be equivalent to input tensor."], ["ReduceMin", "type: INT. Defines behavior if \u2018axes\u2019 is empty. Default behavior with \u2018false\u2019\nis to reduce all axes. When axes is empty and this attribute is set\nto true, input tensor will not be reduced,and the output tensor\nwould be equivalent to input tensor."], ["ReduceProd", "type: INT. Defines behavior if \u2018axes\u2019 is empty. Default behavior with \u2018false\u2019\nis to reduce all axes. When axes is empty and this attribute is set\nto true, input tensor will not be reduced,and the output tensor\nwould be equivalent to input tensor."], ["ReduceSum", "type: INT. Defines behavior if \u2018axes\u2019 is empty. Default behavior with \u2018false\u2019\nis to reduce all axes. When axes is empty and this attribute is set\nto true, input tensor will not be reduced,and the output tensor\nwould be equivalent to input tensor."]], "allowzero": [["Reshape", "type: INT. (Optional) By default, when any value in the \u2018shape\u2019 input is equal\nto zero the corresponding dimension value is copied from the input\ntensor dynamically. allowzero=1 indicates that if any value in the\n\u2018shape\u2019 input is set to zero, the zero value is honored, similar to\nNumPy."]], "gamma": [["Selu", "type: FLOAT. Coefficient of SELU default to 1.05070102214813232421875 (i.e.,\nfloat32 approximation of 1.0507009873554804934193349852946)."]], "end": [["Shape", "type: INT. (Optional) Ending axis for slicing the shape. Negative value means\ncounting dimensions from the back. If omitted, sizes of all axes\nupto (including) the last one will be included."]], "start": [["Shape", "type: INT. (Optional) Starting axis for slicing the shape. Default value is\n0.Negative value means counting dimensions from the back."]], "mode": [["Upsample", "type: STRING. Two interpolation modes: nearest (default), and linear (including\nbilinear, trilinear, etc)"]]}