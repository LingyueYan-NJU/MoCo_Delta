api: torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None,
  norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, _freeze=False,
  device=None, dtype=None)
constraints:
  embedding_dim:
    descp: the size of each embedding vector
    dtype: int
    range:
    - 1
    - 1026
    shape: 1
    structure:
    - integer
  max_norm:
    descp: If given, each embedding vector with norm larger than
    dtype: torch.float
  norm_type:
    default: 2.0
    descp: The p of the p-norm to compute for the 2.
    dtype: torch.float
  num_embeddings:
    descp: size of the dictionary of embeddings
    dtype: int
    range:
    - 1
    - 1024
    shape: 1
    structure:
    - integer
  padding_idx:
    descp: If specified, the entries at
    dtype: int
    range:
    - 1
    - 2
    shape: 1
    structure:
    - integer
  scale_grad_by_freq:
    default: false
    descp: If given, this will scale gradients by the inverse of frequency of the
      words in the mini-batch.
    dtype: torch.bool
  sparse:
    default: false
    descp: If True, gradient w.r.t.
    dtype: torch.bool
descp: A simple lookup table that stores embeddings of a fixed dictionary and size.
inputs:
  optional:
  - padding_idx
  - max_norm
  - norm_type
  - scale_grad_by_freq
  - sparse
  required:
  - num_embeddings
  - embedding_dim
