api: torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0,
  scale_grad_by_freq=False, mode='mean', sparse=False, _weight=None, include_last_offset=False,
  padding_idx=None, device=None, dtype=None)
constraints:
  embedding_dim:
    descp: the size of each embedding vector
    dtype: int
    range:
    - 1
    - 1026
    shape: 1
    structure:
    - integer
  include_last_offset:
    default: false
    descp: if True,
    dtype: torch.bool
  max_norm:
    descp: If given, each embedding vector with norm larger than
    dtype: torch.float
  mode:
    default: mean
    descp: '"sum", "mean" or "max". Specifies the way to reduce the bag. "sum" computes
      the weighted sum, taking "mean" computes the average of the values in the bag,
      "max" computes the max value over each bag.'
    dtype: torch.string
    range:
    - sum
    - max
    - mean
  norm_type:
    default: 2.0
    descp: The p of the p-norm to compute for the 2.
    dtype: torch.float
  num_embeddings:
    descp: size of the dictionary of embeddings
    dtype: int
    range:
    - 1
    - 1024
    shape: 1
    structure:
    - integer
  scale_grad_by_freq:
    default: false
    descp: if given, this will scale gradients by the inverse of frequency of the
      words in the mini-batch.
    dtype: torch.bool
  sparse:
    default: false
    descp: if True, gradient w.r.t. mode="max".
    dtype: torch.bool
descp: "Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating\
  \ the intermediate embeddings."
inputs:
  optional:
  - max_norm
  - norm_type
  - scale_grad_by_freq
  - mode
  - sparse
  - include_last_offset
  required:
  - num_embeddings
  - embedding_dim
